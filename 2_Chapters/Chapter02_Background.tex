\chapter{Background \& Related Work}
\label{ch:background}

% This section is where you will discuss relevant background work, and related works for comparison.
% Ensure that you cite references appropriately, using this as an example~\citep{sample2019}
%
% \section{Background Topic 1}
%
% \section{Background Topic 2}
%
% \section{Related Work}

\section{Intelligent Agents}
\label{sec:agents}

There has been prior research and discussion to determine what constitutes an \textit{agent}.
Though work by \citet{wooldridge_agent_1995} fails to completely define the term,
the authors do propose a set of characteristics that agents must have.
They are summarized by \citet{dignum_intentional_1998} and are as follows.

First, the agent must have autonomy to complete its own actions without the intervention of a human.
This implies that it is making decisions on its own power.
Second, the choices of an agent must be best explained in terms of some form of \textit{intention}.
These intentions can be desires, goals, etc.
An implication of this requirement is that agents cannot be explained through low-level concepts.
Lastly, an agent explained with particular intentions should seem to actually progress their satisfaction.
At the very least, an agent should be \textit{trying} to fulfill its intentions (even if it is doing so poorly).
A system that does not even do this cannot be called an \textit{agent}.

There are many different kinds of agents.
\citet{balke_how_2014} provide a survey of five different kinds of agents.

The first of these are \textit{production rule systems}~\citep{balke_how_2014}.
Production rule systems are symbolic in nature and
consist of three core components:

\begin{itemize}

    \item A set of \textit{rules} (also called \textit{productions}).
        Each rule has the form $ C_i \rightarrow A_i $,
        where $ C_i $ is what is called the \textit{sensory precondition}
        and $ A_i $ is the action to be performed if $ C_i $ is true.
        $ C_i $ is essentially an \textit{if} condition and $ A_i $ is the conditionally executed body.

    \item A knowledge base that stores domain-relevant information about the agent's environment.
        This is called the \textit{working memory}.

    \item A \textit{rule interpreter} that determines which rules apply for the current state of the working memory.
        In the case of rule conflicts, it decides which rule should be executed.

\end{itemize}

% TODO: Consider talking about architecture

Production rule systems are usually implemented in Prolog or Lisp~\citep{balke_how_2014}.

The next class of agents are those ascribing to the \textit{BDI Architecture}~\citep{balke_how_2014}.
The belief-desire-intention (BDI) model combines a philosophical model of human practical reasoning
with several successful applications~\citep{georgeff_belief-desire-intention_1999}.
BDI agents are built on the idea of mental states,
consisting of three items: beliefs, desires, and intentions~\citep{balke_how_2014}.
\textit{Beliefs} refer to an agent's representation of the world.
An agent believes something to be true if and only if the statement is found in its representation of the world.
It is worth noting that an agent's beliefs can be inconsistent with its environment (i.e.~the agent can mistakenly believe a falsehood about its environment).

A BDI agent's \textit{desires} are all possible courses of action that it might want to accomplish in order to reach a goal~\citep{balke_how_2014}.
An agent is not necessarily committed to its desires.
They just play a part in its decision process.
\textit{Intentions}, on the other hand, are commitments to particular courses of action to reach a goal.
Courses of action are also called \textit{plans}.

In addition, BDI agents have a library of plans.
This library consists of pre-computed primitive logic rules to signify which actions contribute to accomplishing which goals.
At each reasoning step, BDI agents search through the plan library to see which plans have a post-condition that matches the currently selected intention
and then sorts them according to relevance.

The third class of agents are those that conform to \textit{normative models}.
They differ from BDI agents in that they are \textit{externally motivated}.
BDI agents are \textit{internally motivated} since all major components (beliefs, goals, and intentions) are internal to the agent.
Normative agents are governed by norms, which are imposed by the agent's external environment.

The last two classes of agents are those with cognitive models~\citep{balke_how_2014}.
They can be divided into those that have \textit{simple} cognitive models and those that have \textit{cognitive architectures}.
The simple cognitive models share similarities with the agent classes presented above
whereas the cognitive architectures are heavily influenced by the structure of the human brain.

\section{Evaluation of Agent Architectures}
\label{sec:agent_evaluation}

Given these five classes of agent architectures, one needs a way of objectively comparing them.
\citet{balke_how_2014} provide five different dimensions on which these architectures can be evaluated.

The first one is the \textit{cognitive dimension}.
This dimension considers what kind of reasoning is able to be performed by an agent.
\citeauthor{balke_how_2014} evaluate architectures on whether agents are \textit{reactive} or \textit{deliberative}.
Reactive agents are those that only respond to stimuli in their environment and typically follow rule-based patterns.
Deliberative agents, on the other hand, actively consider different courses of action and weigh the utility of each.
For example, production rule systems are an example of reactive agents whereas BDI agents are deliberative~\citep{balke_how_2014}.

The second dimension for agent architecture comparisons is the \textit{affective dimension}~\citep{balke_how_2014}.
The affective dimension considers what degree of emotions agents in an architecture are capable of expressing and acting upon.
Research towards this dimension considers how emotions can be triggered, how they influence the decision process, and how changes in decisions affect the system at large.
Most models are incapable of acting upon emotions~\citep{balke_how_2014} with the exception of two extensions of the BDI model~\citep{jiang_ebdi_2007, dignum_towards_2009} and a cognitive model~\citep{urban_pecs_2000}.

A third dimension for comparing architectures is the \textit{social dimension}.
This dimension considers the extent to which agent architectures provide agents the ability to communicate with other agents, to distinguish social relations (such as status), and to understand complex social concepts.
All the normative models discussed by \citet{balke_how_2014}, along with a few others among BDI extensions and cognitive models, have a notion of self, others, and groups.
BDI agents, in the original model, as well as production rule systems have no notion of social concepts or communication.

Fourth, agent architectures can be evaluated according to the extent of formal and social norms that agents can reason over~\citep{balke_how_2014}.

Lastly, architectures can be compared by considering the type of agent learning that the architecture enables for its agents~\citep{balke_how_2014}.
This dimension considers whether agents can improve or expand their knowledge base given environmental observations.
If agents are capable of doing this, it also considers in what ways the knowledge base can be extended (e.g.~whether they adjust the values for pre-existing decision functions or create new decision rules from scratch)~\citep{balke_how_2014}.
Production rule systems and BDI agents (including its variants) do not support any form of learning whereas normative and cognitive model support it to various extents.

A related consideration for agent architectures is its \textit{elaboration tolerance}.
Elaboration tolerance measures the ease of which a knowledge base can be changed to incorporate new facts~\citep{parmar_formalizing_2003}, whether by learning or by a human redesign.
For example, a representation with a high elaboration tolerance might require very little changes to represent additional information whereas a representation with a low elaboration tolerance may need to be completely re-written to address a new scenario.
The concept was first mentioned by \citet{mccarthy_mathematical_1988}, but later formalized by \citet{parmar_formalizing_2003}.

\section{Logic-based Intelligent Agents}
\label{sec:logic_based_agents}

In addition to those mentioned by \citet{balke_how_2014}, there has been research on various logic-based agent architectures.
These agent architectures are based off the notion of an \textit{action language} which, in turn, are based on \textit{transition systems}.

\subsection{Transition Systems}
\label{subsec:transition_systems}

Transition systems (also called \textit{transition diagrams}~\citep{blount_architecture_2013}) can be thought of as analogous to a directed graph that models a discrete dynamic domain.
Discrete dynamic domains are environments that behave according to the following rules~\citep{blount_architecture_2013}:

\begin{itemize}
    \item The environment updates in discrete time steps.
    \item Actions occur instantaneously at each time step and the effects appear immediately at the next time step.
    \item Each domain-relevant property is represented by a function.
        The function maps the property name, called a \textit{fluent}, to its value at a particular time step.
        Fluents with their values are collectively referred to as the agent's \textit{state}.
\end{itemize}

Transition systems, then, are akin to directed graphs where the vertices are possible states of the world and edges are actions that, when performed, transition the world from one state to another.
Their formal definition is as follows:

\begin{definition}
    \label{def:action_signature}
    An action signature $ < \boldsymbol{V}, \boldsymbol{F}, \boldsymbol{A}> $ consists of three sets: a set $ \boldsymbol{V} $ for value names, a set $ \boldsymbol{F} $ of fluent names, and a set $ \boldsymbol{A} $ of action names~\citep{gelfond_action_1998}.
\end{definition}

\begin{definition}
    \label{def:propositional_action_signature}
    An action signature $< \boldsymbol{V}, \boldsymbol{F}, \boldsymbol{A}>$ is \textit{propositional} if $\boldsymbol{V}=\{t,f\}$~\citep{gelfond_action_1998}.
\end{definition}

\begin{definition}
    \label{def:transition_system}
    A transition system of an action signature $ < \boldsymbol{V}, \boldsymbol{F}, \boldsymbol{A}> $ consists of a set $ S $ of states, a function $ V : \boldsymbol{F} \times S\rightarrow \boldsymbol{V} $, and a set of transitions $ R \subseteq S \times \mathcal{P}(\boldsymbol{A}) \times S $.
\end{definition}

$ V(P,s) $ is said to be the value of fluent $ P $ in state $ s $.
The states $ s' $ such that $ <s, A, s'> \in R $ are the possible results of an action $ A $'s execution in state $ s $.

\begin{definition}
    \label{def:action_a_determinism}
    Action $ A $ is \textit{deterministic} in state $ s $ if there is exactly one state $ s' $~\citep{gelfond_action_1998}.
    If $ A $ is \textit{non-deterministic}, then each state $ s' $ \textit{may} be the result of action $ A $~\citep{blount_architecture_2013}.
    $ A $ is \textit{executable} in $ s $ iff there is at least one state $ s' $ where $ <s, A, s'> \in R $.
    When $ \left|A\right|>1 $, $A$ represents the execution of concurrent actions~\citep{gelfond_action_1998, blount_architecture_2013}.
\end{definition}

\begin{definition}
    \label{def:wait}
    When action $A = \emptyset$, $A$ is sometimes denoted as $Wait$~\citep{gelfond_action_1998}.
\end{definition}

\begin{definition}
    \label{def:event}
    The pair $<s, A>$ is sometimes called an \textit{event}~\citep{gelfond_authorization_2008}.
\end{definition}

\begin{definition}
    \label{def:trajectory}
    A path $ <s_0, A_0, s_1, A_1, \dots, A_{\left\{n-1\right\}}, s_n> $ represents a possible \textit{trajectory} of a system with the initial state $ s_0 $ and final state $ s_n $~\citep{blount_architecture_2013}.
\end{definition}

\begin{definition}
    \label{def:propositional_transition_system}
    If the action signature of a transition system is propositional, then the transition system is said to be \textit{propositional}~\citep{gelfond_action_1998}.
\end{definition}

\begin{definition}
    Fluents of a propositional action signature are sometimes referred to as \textit{boolean fluents}.
    Otherwise, there are called \textit{functional fluents}~\citep{chintabathina_modeling_2004}.
\end{definition}

\begin{definition}
    An action $a \in A$ is sometimes called an \textit{elementary action} where $A$ is called a \textit{compound action}~\citep{gelfond_authorization_2008}.
\end{definition}

\subsection{Action Languages}
\label{subsec:action_languages}

While transition systems are useful because of their strong mathematical foundation, specifying complex transition systems is very difficult in practice.
To remedy this, helpful abstractions of transitions systems called \textit{action languages} have been proposed.
Action languages are formal models that describe the effects of actions in a concise format~\citep{gelfond_action_1998}.
There are two kinds of action languages: action description languages and action query languages.
Action description languages describe action effects in terms of a transition system and action query languages state assertions about a transition system~\citep{gelfond_action_1998}.
We will focus only on action description languages and will use the term \textit{action languages} to refer to them.

Many action languages exist, including $ \mathcal{A} $~\citep{gelfond_action_1998}, $ \mathcal{B} $~\citep{gelfond_action_1998}, $ \mathcal{C} $~\citep{gelfond_action_1998}, $ \mathcal{AC} $~\citep{turner_representing_1997}, $ \mathcal{AL} $~\citep{baral_reasoning_2000}, $ \mathcal{ALM} $~\citep{inclezan_modular_2016}.

\subsubsection{Action Language $ \mathcal{A} $}
\label{subsubsec:action_language_a}

First, we will discuss action language $ \mathcal{A} $.
Action language $ \mathcal{A} $ was first proposed by \citet{pednault_formulating_1987}.
$ \mathcal{A} $ models a transition system with the signature $ <\{t, f\}, \boldsymbol{F}, \boldsymbol{A}> $.
It is composed of propositions of the form:
\begin{equation}
    a \textbf{ causes } L \textbf{ if } F
\end{equation}
where $ a \in \boldsymbol{A} $ is an action name, $ L $ is a literal (also known as the head), and $ F \subseteq(\boldsymbol{F} \cup\{\neg f | f \in \boldsymbol{F}\}) $ is a possibly empty conjunction of literals~\citep{gelfond_action_1998}.
A literal is an element of $(\boldsymbol{F} \cup \{\neg f | f \in \boldsymbol{F}\})$.
Since we model a propositional transition system, $L$ denotes $V(L, s') = t$ for a transition from $s$ to $s'$ where $F$ holds in $s$ and $a$ occurs.
Likewise, $\neg L$ denotes $V(L, s')=f$.
In the case $ F $ is empty, it is denoted as $ True $ and can be omitted (along with the preceding ``if'').
A set of these propositions is called an \textit{action description}~\citep{gelfond_action_1998}.

To formally define the semantics of an action description in $\mathcal{A}$, we need to first define an interpretation.

\begin{definition}
    \label{def:interpretation}
    A function $G$ is an \textit{interpretation} of a set $S$ iff $G: S \rightarrow \{t, f\}$~\citep{gelfond_action_1998}.
\end{definition}

\begin{definition}
    Let $ D $ be an action description in $ \mathcal{A} $.
    The transition system $ <S, V, R> $ described by $ D $ is defined as follows~\citep{gelfond_action_1998}:

    \begin{itemize}
        \item $ S $ is the set of all interpretations of $ \boldsymbol{F} $.
        \item $ V(P, s) $ is the value of fluent $P$ in state $s$.
        \item R is the set of all triples $ <s, \{a\}, s'> $ such that $ E(a,s) \subseteq s' \subseteq E(a,s) \cup s $.
    \end{itemize}
    where $ E(a, s) $ is the set of heads $ L $ of all propositions $ a \textbf{ causes } L \textbf{ if } F $ in $ D $ such that $ s $ satisfies $ F $.
    $ E(a, s) $ represents all effects of an action $ a $ in state $ s $~\citep{gelfond_action_1998}.
    Writing $ E\left(a,s\right)\subseteq s' $ constrains $ s' $ such that it must represent states in which the effects of $ a $ are actually present.
    Adding $ \subseteq E\left(a,s\right)\cup s $, allows $ s' $ to equal $ s $.
    It is worth noting that there can only be at most one $ s' $ satisfying the above equation~\citep{gelfond_action_1998}.
    Therefore, $ a $ is deterministic for every state.
\end{definition}

Suppose we had the following action description in $\mathcal{A}$, as given by \citet{gelfond_action_1998}:
\begin{equation}
    \label{eq:action_language_a_example}
    a \textbf{ causes } P \textbf{ if } Q
\end{equation}
In this case, the action description language models the transition system shown in \cref{fig:action_language_a_example}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/Action_Language_A/Example.dot.pdf}
    \caption{Transition system corresponding to \cref{eq:action_language_a_example}}
    \label{fig:action_language_a_example}
\end{figure}

\subsubsection{Action Language $ \mathcal{B} $}
\label{subsubsec:action_language_b}

Action language $ \mathcal{B} $ is an extension of action language $ \mathcal{A} $ [12] that allows the description of actions with indirect effects~\citep{gelfond_action_1998}.
It does so with the addition of so-called \textit{static laws}~\citep{gelfond_action_1998}.
\citet{gelfond_action_1998} contrast static laws to \textit{dynamic laws}, which refer to action descriptions in $\mathcal{A}$.

Like $\mathcal{A}$, $\mathcal{B}$ models a transition system with the signature $<\{t, f\},\boldsymbol{F},\boldsymbol{A}>$.
An action description in $\mathcal{B}$ is a set of static and dynamic laws, which are as follows~\citep{gelfond_action_1998}:

\begin{itemize}
    \item A static law is written in the following form:
        \begin{equation}
            L \textbf{ if } F
        \end{equation}

    \item A dynamic law, like in $\mathcal{A}$, is written in the form:
        \begin{equation}
            a \textbf{ causes } L \textbf{ if } F
        \end{equation}
\end{itemize}

\noindent
where $a \in \boldsymbol{A}$ is the action name, $L \in(\boldsymbol{F} \cup\{\neg f | f \in \boldsymbol{F}\})$ is a literal (or \textit{head}) and $F \subseteq(\boldsymbol{F} \cup\{\neg f | f \in \boldsymbol{F}\})$ is a, possibly empty, conjunction of literals.

Similar to $\mathcal{A}$, if $F$ is empty then it is denoted as $True$ and can be omitted.

$\mathcal{B}$ defines a notion of closure for sets of literals.

\begin{definition}
    \label{def:causation}
    In a dynamic law $a \textbf{ causes } L$, $a$ is said to \textit{cause} $L$.
    Adding a static law $L' \textbf{ if } L$, $a$ is said to \textit{indirectly cause} $L'$.
\end{definition}

\begin{definition}
    ``A set $s$ of literals is \textit{closed} under a set $Z$ of static laws if $s$ includes the head $L$ of every static law $L \textbf{ if } F$ in $Z$ such that $s$ satisfies $F$''~\citep{gelfond_action_1998}.
    Informally, a set $s$ of literals is closed under a set of static laws $Z$ if there are relevant static laws in $Z$ that require every literal in $s$.
    A static law is informally relevant if its set of literals $F$ is satisfied.
\end{definition}

\begin{definition}
    ``The set $C_{n_Z}\left(s\right)$ of \textit{consequences} of [a set] $s$ [of literals] under [a set] $Z$ [of static laws] is the smallest set of literals that contains $s$ and is closed under $Z$.~\citep{gelfond_action_1998}''
    Informally, the consequences of s are the collection of all effects of a particular fluent value (both direct and indirect).
\end{definition}

\begin{definition}
    Let $D$ be an action description in $\mathcal{B}$.
    The transition system $< S, V, R >$ described by $D$ is defined as follows~\citep{gelfond_action_1998}:

    \begin{itemize}
        \item $S$ is the set of all interpretations of $\boldsymbol{F}$ that are closed under the static laws of $D$.
        \item $ V(P, s) $ is the value of fluent $P$ in state $s$.
        \item $R$ is the set of all triples $<s, \{a\}, s'>$ such that
            \begin{equation}
                s'=C_{n_Z}\left(E\left(a,s\right)\cup\left(s \cap s'\right)\right)
            \end{equation}
            $ Z $ is the set of all static laws of $D$ and $E(a, s)$, as before, is the set of heads $L$ of all propositions $a \textbf{ causes } L \textbf{ if } F$ in $D$ such that $s$ satisfies $F$.
    \end{itemize}
\end{definition}

\subsubsection{Action Language $ \mathcal{AL} $}
\label{subsubsec:action_language_al}

$ \mathcal{AL} $ is based on $ \mathcal{A} $ and $ \mathcal{B} $.
It extends these languages with what authors later refer to as \textit{defined fluents}.
These are domain-relevant properties under the closed world assumption~\citep{blount_architecture_2013}.
The closed world assumption states that which cannot be proven true must be false~\citep{reiter_closed_1981}.
It contrasts from the open world assumption, which states only that which is proven is true~\citep{reiter_closed_1981}.

An action description in $ \mathcal{AL} $ is a set of propositions written in the form~\citep{baral_reasoning_2000, blount_architecture_2013}:

\begin{itemize}
    \item Dynamic laws (from Action language $ \mathcal{A} $):
        \begin{equation}
            a \textbf{ causes } L \textbf{ if } F
        \end{equation}

    \item Static laws (from Action language $ \mathcal{B} $):
        \begin{equation}
            L \textbf{ if } F
        \end{equation}

    \item Executability constraints:
        \begin{equation}
            \textbf{ impossible } a_0, a_1, \dots, a_k \textbf{ if } F
        \end{equation}
\end{itemize}

Like in $\mathcal{A}$ and $\mathcal{B}$, $a \in \boldsymbol{A}$ is the action name, $L \in(\boldsymbol{F} \cup\{\neg f | f \in \boldsymbol{F}\}) $ is a literal (or \textit{head}) and $F \subseteq(\boldsymbol{F} \cup\{\neg f | f \in \boldsymbol{F}\})$ is a possibly empty conjunction of literals.

Later work by \citet{gelfond_knowledge_2014} proposes classifications for literals: \textit{statics}, \textit{inertial fluents}, and \textit{defined fluents}.
The term \textit{fluent} has a slightly different meaning in this context.
\citet{gelfond_knowledge_2014} use the term \textit{domain property} to refer to the \textit{fluent names} introduced in our discussion of action signatures.
By \textit{fluent}, \citet{gelfond_knowledge_2014} mean the domain properties that might actually change between states.
They call domain properties that will not change between states \textit{statics}.
It is worth noting that the distinction between these fluents and statics is only useful when considering the implementation of a system using action language $\mathcal{AL}$.
Representing statics as a runtime constant can result in performance improvements.

\citet{gelfond_knowledge_2014} further partition the term \textit{fluent} into \textit{defined fluents} and \textit{inertial fluents}.
Inertial fluents are inspired by the law of inertia in that their value persists from state to state unless an action explicitly changes it (either directly or indirectly).
Defined fluents, on the other hand, are defined in terms of other fluents and do not necessarily persist from state to state.
Due to the constraints on their definition, they cannot be directly caused by an action.


\subsection{The AAA Architecture}
\label{subsec:aaa_architecture}

The first logic-based agent architecture that we will discuss is the \textit{Autonomous Agent Architecture} (AAA) architecture~\citep{balduccini_aaa_2008}.
The AAA architecture is based on work by Gelfond and collaborators~\citep{baral_reasoning_2000,balduccini_diagnostic_2003, balduccini_answer_2006,balduccini_learning_2007}.
As it is a logic-based approach, it makes certain assumptions about the environment in which an agent constructed in its architecture will be operating.
They must be met in order for the architecture to be applicable.
The assumptions are as follows~\citep{balduccini_aaa_2008}:

\begin{itemize}
    \item The world (the agent along with its environment) can be modeled in a transition system.
    \item The agent is competent in interacting with its environment (e.g.~the agent is capable of making correct observations, remembering them, and can perform actions).
    \item Normally, the agent can observe all relevant exogenous events (i.e.~events that are not an effect of an action the agent performed).
\end{itemize}

Given that these assumptions are met, an agent in the AAA architecture performs the following loop once for every time step of the environment~\citep{balduccini_aaa_2008}:

\begin{enumerate}
    \item Observe the world, explain observations, and update knowledge base.
    \item Select an appropriate goal $G$.
    \item Find a sequence of compound actions (called a \textit{plan}) that achieve $G$.
    \item Execute first action in plan and update knowledge base.
    \item Repeat (i.e.~go to step 1).
\end{enumerate}

This loop is known as the \textit{Observe-Think-Act} loop~\citep{balduccini_aaa_2008}.

In step 1, the agent observes the world and updates its knowledge base.
This knowledge base is encoded in the Answer Set Prolog (ASP) language, which allows for the representation of many different forms of knowledge [18] (ASP will be discussed later in \cref{sec:asp}).
The agent stores the current state of the environment in this knowledge base as well as a history of previous observations and actions (more will be mentioned in \cref{subsubsec:domain_history})~\citep{balduccini_aaa_2008}.

There are two approaches to describing the effects of an agent's actions.
The first is to use action languages to formally denote the action effects, then to translate them into ASP for executability.
The second is to author them directly in ASP.
\citet{balduccini_aaa_2008} choose to do the latter.

It is worth noting that the agent makes no commitment to plans it found in previous loop iterations.
Each loop iteration searches for a plan irrespective to the plan that was partially executed in the previous iteration.
This makes the Observe-Think-Act loop somewhat greedy in that it chooses the best plan as can be foreseen at every time step.
In more recent work~\citep{blount_towards_2014}, \citeauthor{blount_towards_2014} see this as a limitation and build the $\mathcal{AIA}$ architecture to address it.

\subsection{The $\mathcal{AIA}$ Architecture}
\label{subsec:aia_architecture}

As stated above, the \textit{Architecture for Intentional Agents} ($\mathcal{AIA}$) is an agent architecture that borrows significantly from the AAA architecture~\citep{blount_towards_2014}.
It extends the AAA architecture by representing the possibility for action failure.
In contrast to the AAA architecture, the agent \textit{attempts} to perform an action during its control loop but may find that it is unable to do so.
In this case, the action is \textit{non-executable}~\citep{blount_towards_2014}.

The $\mathcal{AIA}$ architecture shares a number of similar assumptions with the AAA architecture~\citep{blount_towards_2014}:

\begin{itemize}
    \item The agent is capable of making correct observations.
    \item When the agent attempts to perform an action, the action occurs only when it is executable.
        Otherwise, nothing happens.
    \item The agent remembers its observations of the environment (as well as its prior attempts to take actions).
    \item Normally, the agent observes all relevant exogenous events.
\end{itemize}

However, a few assumptions differ from the AAA architecture:

\begin{itemize}
    \item The agent selects a single goal only when it does not have one and focuses on achieving it.
    \item The world must be modeled in an \textit{intentional system description} of $\mathcal{AL}$.
\end{itemize}

\subsubsection{Intentional System Description of $\mathcal{AL}$}
\label{subsubsec:intentional_action_language_al}

An intentional system description of $ \mathcal{AL} $ is an extension of $ \mathcal{AL} $ proposed by \citet{blount_architecture_2013}.
This extension is created to address the limitations of the AAA architecture, in which plans are not persisted across iterations of the Observe-Think-Act loop.
In order to persist them, plans are pre-computed and stored as a set of statics.
In the intentional system description of $\mathcal{AL}$, these pre-computed plans are encapsulated in a construct called an \textit{activity}.
Activities are the set of the following statics~\citep{blount_architecture_2013, blount_towards_2014}:

\begin{gather}
    activity(M). \\
    component(M, 1, C_1). \\
    component(M, 2, C_2). \\
    \dots \\
    component(M, L, C_L). \\
    length(M, L). \\
    goal(M, G).
\end{gather}

\noindent
where $M$ is a unique identifier for the activity, $C_1, C_2, \dots, C_L$ are the \nth{1}, \nth{2}, \dots, L\textsuperscript{th} \textit{component} of the activity, and $G$ is the goal that $C_1,C_2,\dots,C_L$ achieves.
In contrast to the AAA architecture, a component $C_k$ can either refer to an action or another activity $M'$ by its unique identifier.
In the latter case, $M'$ is a \textit{child activity} of $M$.
An activity cannot be a child (or descendant) of itself (i.e.~activities must be acyclic).
It is also worth noting that activities permit the execution of actions concurrently.

Since activities are pre-computed, there can be an exponentially large amount of unique activities represented by statics.
This can be detrimental to running an implementation so to avoid this, \citet{blount_architecture_2013} only represents activities that are deemed initially relevant and generates the rest on demand.

The intentional system description of $\mathcal{AL}$ uses fluents to keep track of the agent's progress in the current activity.
These fluents define the agent's \textit{mental state} and are updated through \textit{mental actions}.
Mental actions contrast from \textit{physical actions} in that mental actions only affect the internal state of the agent whereas physical actions affect the agent's environment.
The effects of mental actions are defined in a set of propositions called the Theory of Intentions ($\mathcal{TI}$)~\citep{blount_towards_2014}.

\subsubsection{Theory of Intentions ($\mathcal{TI}$)}
\label{subsubsec:theory_of_intentions}

The Theory of Intentions ($\mathcal{TI}$) is a framework of static and dynamic laws that maintain an agent's mental state.
Elements of the agent's mental state include the currently selected goal $G$, stored in the $active\_goal\left(G\right)$ inertial fluent, the current planned activity, stored in the $status(M, k)$ inertial fluent~\citep{blount_towards_2014}.
When either a goal is selected or an activity is planned, they are said to be \textit{intended}.

$\mathcal{TI}$ updates the agent's mental states through mental actions.
For example, the action $start(M)$ initiates the agent's intention to execute activity $M$ and $stop(M)$ terminates the agent's intention to execute $M$~\citep{blount_towards_2014}.
Though most actions are executed by the agent itself, some must be executed by the agent's controller.
There are two such actions that are exogenous to the agent.
The exogenous mental action $select(G)$ causes the agent to intend to achieve goal $G$~\citep{blount_towards_2014}.
Likewise, the exogenous mental action $abandon(G)$ causes the agent to cease its intent to achieve goal $G$~\citep{blount_towards_2014}.
These actions are necessarily exogenous because, in general, a goal-driven agent must be told to achieve at least one goal beforehand.

\subsubsection{Domain History}
\label{subsubsec:domain_history}

Previously, we mentioned that a differing aspect of the $\mathcal{AIA}$ architecture is that an agent can attempt an action but fail to do so.
This has significant implications on how the agent's history is represented.
Suppose an agent in the AAA architecture has the following action description (borrowed from~\citep{blount_towards_2014}):

\begin{gather}
    a_1 \textbf{ causes } f \\
    a_2 \textbf{ causes } g \textbf{ if } f, p \\
    b \textbf{ causes } \neg p
\end{gather}

Suppose the agent has the following history\footnotemark:
\begin{equation}
    \Gamma=\{obs\left(p,true,0\right),obs\left(f,false,0\right),hpd\left(a_{1,}0\right),hpd\left(a_{2,}1\right),obs\left(g,false,2\right)\}
\end{equation}
where $obs(F, V, I)$ denotes that the fluent $F$ was observed to have value $V$ at time step $I$ and $hpd(a, I)$ that action $a$ happened at time step $I$.

\footnotetext{
    \citet{blount_towards_2014} uses a slightly different notation than what \citet{balduccini_aaa_2008} introduced.
    \citet{balduccini_aaa_2008} denote the $obs$ predicate as $obs(FluentName, TimeStep, FluentValue)$ whereas \citet{blount_towards_2014} writes $obs(FluentName, FluentValue, TimeStep)$ to denote the same history.
    Since these notations are equivalent and merely a matter of preference, this paper will use the style introduced by \citet{blount_towards_2014}.
}

This history is inconsistent, since the execution of action $a_2$ should have caused $g$ ($g$ is still false afterwards).
The $\mathcal{AIA}$ architecture avoids this inconsistency by adding two history statements.
When executing an action $a$ at time step $I$, the agent records its attempt to do so with $attempt(a, I)$.
During the $I+1$ iteration of the control loop, the agent records $hpd(a, I)$ if the effects of the previous action are observed, otherwise $\neg hpd(a, I)$.
Thus, an agent in the $\mathcal{AIA}$ architecture will have the following history for the previous example

\begin{multline}
    \Gamma=\{
        obs\left(p,true,0\right),obs\left(f,false,0\right),attempt(a_1, 0),hpd\left(a_{1,}0\right) \\
        attempt(a_2, 1), \neg hpd\left(a_{2,}1\right),obs\left(g,false,2\right)
    \}
\end{multline}

This history is consistent.

\subsubsection{Control Loop}
\label{subsubsec:control_loop}

An agent in the $\mathcal{AIA}$ architecture performs the following loop:

\begin{enumerate}
    \item Interpret observations.
    \item Find intended action $A$.
    \item Attempt to perform $A$;
        record this attempt in history.
    \item Observe the world;
        record observations in history.
    \item Repeat (i.e.~go to step 1).
\end{enumerate}

\subsubsection{Examples}
\label{subsubsec:aia_examples}

To demonstrate the execution of an agent in the $\mathcal{AIA}$ architecture, we will consider an example given by \citet{blount_architecture_2013}.
Suppose that we have an agent named Bob in a workspace with a colleague named John.
The workspace in which they are located has four consecutive rooms $r_1$, $r_2$, $r_3$, $r_4$ with a door connecting each adjacent room.
Both Bob and John are free to \textit{move} from their current room to an adjoined one.
The only exception is that the door joining $r_3$ and $r_4$ has a lock and must be in an unlocked state in order for anything to pass through it.
Both Bob and John can \textit{lock} and \textit{unlock} the door when next to it.
A visual description of this example is shown in \cref{fig:aia_example}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/AIA_Architecture/Example_1}
    \caption{Visual depiction of $\mathcal{AIA}$ example}
    \label{fig:aia_example}
\end{figure}

\subsubsection{Scenario 1: Planning to Achieve Goal~\citep{blount_towards_2014}}
\label{subsubsec:aia_scenario_1}

Initially, Bob knows that he is currently located in $r_1$ and John is located in $r_3$.
Suppose Bob is given an order to meet with John.
This is represented as the occurrence of an exogenous mental action $select(meet(Bob, John))$, which eventually causes the $active\_goal(meet(Bob, John))$ fluent to be true in the agent's mental state.
In the first iteration of the $\mathcal{AIA}$ control loop, the agent will:

\begin{enumerate}
    % TODO: Update this bullet
    \item Interpret its prior observations, of which there are none.
    \item Find an intended action $A$.
        Since there is not yet an intended goal, the agent will choose to execute the \textit{wait} action, which has no physical or mental effects.
    \item The agent will attempt to execute the \textit{wait} action and record this attempt in its history.
    \item The agent observes the occurrence of the $select(meet(Bob, John))$ exogenous action and record it.
\end{enumerate}

Then, in the second iteration of the loop, the agent will interpret its observations.
According to the $\mathcal{TI}$ laws, this will cause the $active\_goal(meet(Bob, John))$ fluent to be true.
Now, when it will search for an activity which has been described to achieve this goal.
Suppose it chooses the activity $<1, [move(Bob, r_1,r_2), move(Bob, r_2, r_3)],meet(Bob,John)>$.
This is represented by the following inertial fluent $status(1,0)$.
The rest of the second iteration is as follows:

\begin{enumerate}
    \setcounter{enumi}{1}
    \item Find intended action $A$.
        This is the $k + 1$ action of the activity where $status(1, k)$.
        In this case, $A$ is $\{move(Bob, r_1, r_2)\}$.
    \item Execute $move(Bob, r_1, r_2)$.
        Record $attempt(move(Bob, r_1, r_2, 1)$ in history.
    \item Observe that Bob is in $r_2$.
        Record $hpd(move(Bob, r_1, r_2),1)$ in history.
\end{enumerate}

When interpreting its observations in the third iteration of the loop, it will update $status(1,0)$ to $status(1,1)$.
The agent in following steps will intend to execute $move(Bob, r_2, r_3)$, attempt to execute it and store $attempt(move(Bob, r_2, r_3))$ in history, and observe that Bob is in $r_3$ and that it has met with John.
Having accomplished its goal, the $active\_goal(meet(Bob, John))$ fluent is set to false and the agent performs the \textit{wait} action until it is given another goal.

\subsubsection{Scenario 2: Serendipitous Achievement of Goal~\citep{blount_towards_2014}}
\label{subsubsec:aia_scenario_2}

Let us rewind the previous scenario such that the agent Bob is currently attempting $move(Bob, r_1, r_2)$.
Suppose that, during the same time step, the following exogenous action occurs $move(John, r_3, r_2)$.
When Bob observes and records the world, it will record $hpd(move(Bob, r_1, r_2), 1)$ and $hpd(move(John,r_3,r_2), 1)$.
In the next iteration of the control loop, Bob will interpret his observations and derive the $meet(Bob, John)$ fluent to be true.
In this case, Bob's goal to achieve $meet(Bob, John)$ is serendipitously satisfied.
Since Bob wants to maintain this state, it will intend to perform action $stop(1)$ to halt the execution of activity $1$.
Bob will attempt to perform $stop(1)$ while updating the history.
Bob will observe and record $hpd(stop(1), 2)$, at which point Bob will choose to wait indefinitely.

\subsubsection{Scenario 3: Replanning~\citep{blount_towards_2014}}
\label{subsubsec:aia_scenario_3}

Let us again rewind Scenario 1 such that Bob is currently attempting $move(Bob, r_1, r_2)$.
Suppose that, during the same time step, the following exogenous action occurs $move(John, r_3, r_4)$.
When Bob interprets this observation, it will recognize that activity $1$ is \textit{futile} (i.e.~it will not satisfy its goal) and so intend to stop activity $1$.
Bob will attempt to perform $stop(1)$, observe that $stop(1)$ has occurred, re-interpret its observations, and search for a new action to perform.
In this step, Bob will look for an activity that satisfies the goal $meet(Bob, John)$.
Suppose it finds the following activity $<2, [move(Bob, r_2,r_3), move(Bob, r_3,r_4)],meet(Bob,John)>$.
Then Bob will intend to perform $start(2)$.
After Bob attempts to perform $start(2)$, it will observe that $start(2)$ happened, interpret its observations, then search for an intended action.
Bob will intend to perform $move(Bob, r_2,r_3)$, attempt to perform it, observe that it happened, interpret its observations, and so on and so forth.
Assuming that $d_{34}$ is unlocked, the scenario will finish like in Scenario 1.

\subsubsection{Scenario 4: Abandoning Goal~\citep{blount_towards_2014}}
\label{subsubsec:aia_scenario_4}

Moving back to Scenario 1 where Bob is currently attempting $move(Bob, r_1, r_2)$.
Suppose that, during the same time step, the following exogenous action occurs $abandon(meet(Bob, John))$.
Then, when observing this action and interpreting it, the $\mathcal{TI}$ laws will cause $active\_goal(meet(Bob, John))$ fluent to be false.
Subsequently, Bob will intend to stop the current activity (by performing $stop(1)$), attempt to perform $stop(1)$, observe, interpret, and intend to perform \textit{wait} in all future loops.

\subsubsection{Scenario 5: Diagnosis~\citep{blount_towards_2014}}
\label{subsubsec:aia_scenario_5}

Finally, let us reset to the moment in Scenario 1 where Bob is performing $move(Bob, r_2, r_3)$.
When later observing the world, suppose Bob does not find John there in $r_3$.
After recording this observation in the history, Bob then interprets its observations.
Since activity 1 does not achieve the goal $meet(Bob, John)$, Bob begins diagnostic reasoning over the history to figure out how this could be possible.
Looking back in history, there are three possibilities when John could have moved: when Bob performed $start(1)$, when Bob performed $move(Bob, r_1, r_2)$, and when Bob performed $move(Bob, r_2, r_3)$.
Assuming John did not pass Bob while moving between rooms, John must have moved to $r_4$.
In this case, Bob must start an activity $<3, [move(Bob, r_3, r_4)],meet(Bob,John)>$ to move from $r_3$ to $r_4$.
However, before it can do so, it must first perform stop(1) before performing start(3).
Hence, it takes three more iterations of the control loop to move to $r_4$ ($stop(1)$, $start(3)$, $move(Bob, r_3,r_4)$) and one final iteration to end the activity ($stop(3)$).

\section{Logic-based Approaches to Representing and Reasoning over Agent Policies}
\label{sec:policies}

Though the agent architectures discussed thus far allow agents to be fully autonomous, real-world applications may require agents to behave in particular ways.
For example, a controller may want to require an agent to follow certain ethical constraints and may choose to penalize an agent when acting in violation of them.
Thus, it is necessary to introduce a discussion on policies for agent behavior and a formalism with which agents can deduce the compliance of their actions.

\subsection{Language $\mathcal{APL}$}
\label{subsec:apl}

\citet{gelfond_authorization_2008} introduce the Authorization Policy Language ($\mathcal{APL}$) to represent authorization policies.
An authorization policy is a set of conditions that denote whether an agent's action is permitted or not permitted~\citep{gelfond_authorization_2008}.
In contrast to the languages discussed previously, $\mathcal{APL}$ is not an action language in and of itself.
$\mathcal{APL}$ describes only the conditions under which actions are permitted or forbidden.
It works in conjunction with action languages such as $\mathcal{AL}$.

\subsubsection{Definition}

Similar to the action languages, $\mathcal{APL}$ requires that the agent's environment be modeled by a transition system $T=<\boldsymbol{V},\boldsymbol{F},\boldsymbol{A}>$.
Recall that $T$ contains all trajectories of the underlying system.
An agent's policy $P$ is the subset of the trajectories of $T$ that are desired by the agent's controller.

\citet{gelfond_authorization_2008} provide a mechanism for describing a policy $P$ by means of \textit{authorization} policy statements.
Thus, a policy $P$ is the set of these statements.
These policies are transcribed using static laws in a similar form to action language $\mathcal{AL}$.

\begin{definition}
    \label{def:authorization_statements}
    Authorization policy statements are static laws of the form~\citep{gelfond_authorization_2008}:

    \begin{gather}
        permitted\left(a\right) \textbf{ if } F \\
        \neg permitted\left(a\right) \textbf{ if } F \\
        \textbf{normally } permitted(a) \textbf{ if } F \\
        \textbf{normally } \neg permitted(a) \textbf{ if } F
    \end{gather}

    \noindent
    where $a \in \boldsymbol{A}$ is an action and $F$ is a, possibly empty, conjunction of fluents, actions, or their negations.
    Formally, $F$ is as follows for a state $s$:\footnotemark
    \begin{multline}
        F\subseteq\{V(f,s)=y | (f, y) \in \boldsymbol{F} \times \boldsymbol{V}\} \cup \boldsymbol{A} \cup \{\neg a | a \in \boldsymbol{A}\} \\ \cup \{permitted(a) | a \in \boldsymbol{A}\} \cup \{\neg permitted(a) | a \in \boldsymbol{A}\}
    \end{multline}

    \footnotetext{
        \citet{gelfond_authorization_2008} use the syntax $f = y$ to denote $V(f, s)=y$ for the current state $s$.
        Usages of this new syntax have been translated in terms of the notation already introduced in this paper so as to highlight the connection between previous concepts.
    }
\end{definition}

\begin{definition}
    \label{def:permission}
    \label{def:denial}
    ~

    \begin{itemize}
        \item A \textit{permission} is a statement of the form $permitted(a)$~\citep{gelfond_authorization_2008}.
        \item A \textit{denial} is a statement of the form $\neg permitted(a)$~\citep{gelfond_authorization_2008}.
    \end{itemize}
\end{definition}

\begin{definition}
    \label{def:strict_authorization_statements}
    A \textit{strict} authorization policy statement is a statement of the form~\citep{gelfond_authorization_2008}:
    \begin{gather*}
        permitted\left(a\right) \textbf{ if } F \\
        \neg permitted\left(a\right) \textbf{ if } F
    \end{gather*}
\end{definition}

\begin{definition}
    \label{def:defeasible_authorization_statements}
    A \textit{defeasible} authorization policy statement is a statement of the form~\citep{gelfond_authorization_2008}:
    \begin{gather*}
        \textbf{normally } permitted(a) \textbf{ if } F \\
        \textbf{normally } \neg permitted(a) \textbf{ if } F
    \end{gather*}
\end{definition}

Defeasible authorization policy statements permit exceptions when they are in conflict with other authorization statements of higher importance.
To encode the relative hierarchy of defeasible statements, \citet{gelfond_authorization_2008} provide an option to label defeasible statements with terms $d_1,d_2,\dots,d_n$ and use the static $prefer\left(d_i,d_j\right)$ to prioritize statement $d_i$ over statement $d_j$.
Thus, defeasible authorization policy statements can be written in the following form~\citep{gelfond_authorization_2008}.
\begin{gather}
    d_i: \textbf{normally } permitted(a) \textbf{ if } F \\
    d_j: \textbf{normally } \neg permitted(a) \textbf{ if } F \\
    prefer(d_i, d_j)
\end{gather}
If a defeasible authorization policy statement is not referenced in a $prefer$ static, then its corresponding label is optional.

\subsubsection{Semantics}

To explain the semantics of $\mathcal{APL}$, we will borrow an example policy from \citet{gelfond_authorization_2008}.
Suppose in a military context, military personnel are given the following rules:

\begin{itemize}
    \item A military officer is not allowed to command a mission he authorized.
    \item A colonel is allowed to command a mission he authorized.
    \item A military observer is never allowed to authorize a mission.
\end{itemize}

For reference, information on military ranking is provided below:

\begin{itemize}
    \item A colonel is a high-ranking military officer.
    \item An officer of any rank outranks an observer.
\end{itemize}

Suppose we have a transition system $T=<\boldsymbol{V}=\{t,f\},\boldsymbol{F},\boldsymbol{A}>$, where
\begin{gather}
    \boldsymbol{A}=\{authorize(C,M),\ assume\_command(C, M)\} \\
    \boldsymbol{F}=\{authorized(C,M),\ commands(C,M),\ colonel(C),\ observer(C)\}
\end{gather}
and $M$ and $C$ are variables that range over all missions and commanders, respectively.

As a policy designer, we might interpret the first statement as defeasible as there might be a rare scenario where an officer might be forced to command his own mission.
So, we might write:
\[
\textbf{normally } \neg permitted(assume\_command(C, M)) \textbf{ if } authorized(C, M)
\]
\citet{gelfond_authorization_2008} interpret the second statement as defeasible as well.
Thus, we could write:
\[
\textbf{normally } permitted(assume\_command(C, M)) \textbf{ if } colonel(C)
\]

It is important to note that our policy is ambiguous at this point.
In the case a colonel authorized a mission and begins to command it, he would be both compliant and non-compliant.
To clarify, we will add labels to the previous statements then prefer honoring the second statement over the first.
\begin{gather}
    d_1(C, M): \textbf{normally } \neg permitted(assume\_command(C, M)) \textbf{ if } authorized(C, M) \label{eq:apl_example_policy_1} \\
    d_2(C, M): \textbf{normally } permitted(assume\_command(C, M)) \textbf{ if } colonel(C) \label{eq:apl_example_policy_2} \\
    prefer(d_2(C,M),d_1(C,M)) \label{eq:apl_example_policy_3}
\end{gather}

Lastly, the third policy statement can be represented with a strict authorization statement.
\begin{equation}
    \label{eq:apl_example_policy_4}
    \neg permitted(authorize(C, M)) \textbf{ if } observer(C)
\end{equation}

In total, our policy $P$ be the set of the above two statement blocks (\cref{eq:apl_example_policy_1,eq:apl_example_policy_2,eq:apl_example_policy_3,eq:apl_example_policy_4}).

\subsubsection{Compliance}

Informally, a set of actions $A$ occurring at a transition system state $s$ are \textit{strongly compliant} with a policy $P$ if all $a \in A$ are known to be permitted using rules in $P$.
$A$ is \textit{non-compliant} with $P$ if any of $a \in A$ are known to be not permitted using rules in $P$.
Otherwise, if $P$ is unclear and does not specify whether an action $a \in A$ is permitted or not, then the set of action $A$ is \textit{weakly compliant}.

Computing compliance with $P$ is reduced to the problem of finding satisfiable answer sets using ASP (See~\cref{sec:asp}) when $P$ is translated into ASP rules.
Compliance with $P$ at a state $s$ can be computed with full or partial knowledge of the fluent values at $s$ as well as with a description of a past state $s_0$ and a history of actions.
We direct the curious reader to \cref{appendix:apl_compliance} for more detail.

\subsection{Language $\mathcal{AOPL}$}
\label{subsec:aopl}

In addition to introducing language $\mathcal{APL}$, \citet{gelfond_authorization_2008} introduce the Authorization and Obligation Policy Language ($\mathcal{AOPL}$) to consider \textit{obligation policies} in addition to authorization policies.
Authorization policies describe what an agent can do whereas obligation policies describe what an agent must do.

\subsubsection{Definition}

Language $\mathcal{AOPL}$ uses $\mathcal{APL}$ as its basis and shares its underlying assumptions, such as requiring the agent's environment be modeled by a transition system $T=<\boldsymbol{V}, \boldsymbol{F}, \boldsymbol{A}>$ and that a policy $P$ is a subset of the trajectories of $T$ that are preferable to the agent's controller.

$\mathcal{AOPL}$ adds an $obl(a)$ statement to $\mathcal{APL}$.
If $obl(a)$ is true, the policy $P$ dictates that the agent must execute $a$ in the current state.
If $obl(\neg a)$ is true, then $P$ dictates that the agent must not execute $a$ in the current state.
Also, if $\neg obl(a)$ is true, then $P$ does not require the agent to execute $a$ in the current state (i.e. it is free to choose to execute $a$ or to not execute $a$).
Likewise, for $\neg obl(\neg a)$.

\begin{definition}
    Obligation policy statements are static laws of the form~\citep{gelfond_authorization_2008}:
    \begin{gather}
        obl\left(h\right) \textbf{ if } F \\
        \neg obl\left(h\right) \textbf{ if } F \\
        d_i: \textbf{normally } obl(h) \textbf{ if } F \\
        d_j: \textbf{normally } \neg obl(h) \textbf{ if } F \\
        prefer(d_i, d_j)
    \end{gather}
    where $h \in \boldsymbol{A} \cup \{\neg a | a \in \boldsymbol{A}\}$ is a \textit{happening} and $F$ is a, possibly empty, conjunction of fluents, actions, or their negations.
    Formally, $F$ is as follows for a state $s$:
    \begin{equation}
        F\subseteq\{V(f,s)=y | (f, y) \in \boldsymbol{F} \times \boldsymbol{V}\} \cup \boldsymbol{A} \cup \{\neg a | a \in \boldsymbol{A}\} \cup \{obl(h), \neg obl(h)\}
    \end{equation}
\end{definition}

\subsubsection{Compliance}

Like $\mathcal{APL}$, compliance with an $\mathcal{AOPL}$ policy $P$ is computed by translating the rules of $P$ into ASP and finding satisfiable answer sets.
Informally, a set of actions $A$ is compliant with an obligation policy $P$ if, where there is $obl(a)$ or $obl(\neg a)$, there is $a \in A$ and $a \not \in A$ respectively.
Given an arbitrary $\mathcal{AOPL}$ policy (with authorization and obligation policy statements), $A$ is \textit{strongly compliant} if it is strongly compliant with its authorization policy and compliant with its obligation policy.
Likewise for \textit{weak compliance} and \textit{non-compliance}.
Note that $A$ can be strongly, weakly, or non-compliant with an authorization policy, $A$ can only be compliant or non-compliant with an obligation policy.
For formal definitions, we direct the curious reader to \cref{appendix:aopl_compliance}.

\subsubsection{Obligation Example}

To demonstrate the compliance of a policy $P$ in $\mathcal{AOPL}$, \citet{gelfond_authorization_2008} present an example.

Suppose a university professor were to encode his classroom policies into $\mathcal{AOPL}$.
He might have requirements such as:

\begin{itemize}
    \item Students are expected to not miss class.
    \item Students should submit homework on time.
    \item Students should do their homework independently.
\end{itemize}

Before we encode these obligations, we need to first define the transition system for this scenario.
Suppose the environment can be modeled by a transition system $T$ where\footnotemark:

\begin{equation}
    T=<\boldsymbol{V}=\{t,f\}, \boldsymbol{F}, \boldsymbol{A}>
\end{equation}
\begin{multline}
    \boldsymbol{F} = \{enrolled(Student, Class), meeting(Class, MeetingNum), \\
        due\_date(meeting(Class, MeetingNum), assignment(Class, AssignmentNum)), \\
        family\_emergency(Student, meeting(Class, MeetingNum)), \\
        religious\_holiday(meeting(Class, MeetingNum))\}
\end{multline}
\begin{multline}
    \boldsymbol{A} = \{attend(Student, meeting(Class, MeetingNum)), \\
        submit(Student, assignment(Class, AssignmentNum), meeting(Class, MeetingNum)), \\
        accept\_unauthorized\_help(Student)\}
\end{multline}
and $Student$, $Class$, $AssignmentNum$, and $MeetingNum$ are variables that range over their respective values.

\footnotetext{
    The following definitions are only implicit in the work by \citet{gelfond_authorization_2008}.
}

Now that we have a transition system defined, we are ready to transcribe the verbal requirements into policy strict/defeasible statements in $\mathcal{AOPL}$.
\citet{gelfond_authorization_2008} envision possible exceptions to the first rule and, in order to maintain elaboration tolerance, express it with a defeasible obligation statement:
\begin{equation}
    \label{eq:aopl_example_line_1}
    d_1(S,C,N): \textbf{ normally } obl(attend(S, meeting(C, N))) \textbf{ if } enrolled(S, C).
\end{equation}

\citet{gelfond_authorization_2008} assume that such an implicit exception may be ``Students do not need to attend class during family emergencies.''
Its encoding would be as follows:
\begin{equation}
    \label{eq:aopl_example_line_2}
    \neg obl(attend(S,meeting(C, N))) \textbf{ if } family\_emergency(S,meeting(C, N)).
\end{equation}

Similarly, they suggest that another exception may be ``Students should not attend class on a religious holiday.''
Its encoding might be as follows:
\begin{multline}
    \label{eq:aopl_example_line_3}
    d_2(S,C,N): \textbf{ normally } obl(\neg attend(S, meeting(C, N))) \\
        \textbf{ if } religious\_holiday(meeting(C, N)).
\end{multline}

\citet{gelfond_authorization_2008} note that writing this rule as defeasible allows the designer to decide whether religious obligations overrule secular requirements.
If he deems that they do, he may write:
\begin{equation}
    \label{eq:aopl_example_line_4}
    prefer(d_2(S,C,N),d_1(S,C,N))
\end{equation}
Otherwise, he may write:
\begin{equation}
    \label{eq:aopl_example_line_5}
    prefer(d_1(S,C,N), d_2(S,C,N))
\end{equation}

While it is possible to omit such a statement from the policy description, doing so would produce an ambiguous policy that would allow an agent to choose which rule it would prefer to uphold.
\citet{gelfond_authorization_2008} do not recommend allowing this behavior.

The second rule may be seen as defeasible to encompass assignment extensions.
\begin{multline}
    \label{eq:aopl_example_line_6}
    d_3(S,C,A,N): \textbf{ normally } obl(submit(S,assignment(C,A),meeting(C,N)) \\ \textbf{ if } enrolled(S,C), due\_date(meeting(C,N), assignment(C,A)).
\end{multline}
Lastly, since cheating is never permissible:
\begin{equation}
    \label{eq:aopl_example_line_7}
    obl(\neg accept\_unauthorized\_help(S)).
\end{equation}

We have substantially expanded the discussion of this example from what was represented by \citet{gelfond_authorization_2008}, regarding both the amount of formal definitions and the overall depth of detail.
We redirect the curious reader to \cref{appendix:aopl_example}.

\section{Answer Set Programming}
\label{sec:asp}

\textit{Answer Set Programming} (ASP) is a form of declarative programming geared towards quickly searching NP problems~\citep{vladimir_lifschitz_what_2008}.
It has its foundations in Knowledge Representation and Reasoning, Logic Programming, Databases, and Boolean Constraint Solving~\citep{gebser_potassco_2011}.
Though its syntax resembles that of Prolog, ASP's execution is very different and resembles that of Satisfiability Testing (SAT)~\citep{gebser_potassco_2011,vladimir_lifschitz_what_2008}.
Its execution comes in two stages: grounding and solving~\citep{gebser_potassco_2011}.

The language itself is called \textit{Answer Set Prolog} (or \textit{AnsProlog*} or \textit{A-Prolog} in older literature~\citep{baral_answer_2004}) whereas the paradigm as a whole is called \textit{Answer Set Programming}.

% It is based on stable model semantics~\citep{gelfond_stable_1988}.

Below we will briefly define the syntax of ASP.
For a more thorough definition, we direct the reader to \citet{calimeri_asp-core-2_2020,gebser_abstract_2015}.

\begin{definition}
    A \textit{signature} is a four tuple $\Sigma = <\mathcal{O}, \mathcal{F}, \mathcal{P}, \mathcal{V}>$, where $\mathcal{O}$, $\mathcal{F}$, $\mathcal{P}$, $\mathcal{V}$ are mutually disjoint sets of objects, functions, predicates, and variables, respectively~\citep{blount_architecture_2013}.
\end{definition}

\begin{definition}
    A \textit{term} over a signature $\Sigma$ consists of any of the following~\citep{blount_architecture_2013}:
    \begin{itemize}
        \item Variables
        \item Object constants
        \item $f(t_1, t_2, \dots, t_n)$, if $t_1, t_2, \dots, t_n$ are terms and $f$ is a function symbol with arity $n$
    \end{itemize}
    For simplicity, we use infix notation for arithmetic expressions (e.g. $1+2$ instead of $+(1, 2)$).
\end{definition}

% \begin{definition}
%     An \textit{atom} is
% \end{definition}

\begin{definition}
    A \textit{literal} is either an atom $a$ or its negation $\neg a$~\citep{balduccini_asp_2011}.
\end{definition}

\begin{definition}
    A term/literal is called \textit{ground} if it does not contain any variables or arithmetic functions~\citep{blount_architecture_2013}.
\end{definition}

\begin{definition}
    A \textit{rule} is a statement of the form~\citep{balduccini_asp_2011}:
    \begin{equation}
    h_1 \lor h_2 \lor \dots \lor h_k \leftarrow l_1, l_2, \dots, l_m, \textbf{ not } l_{m+1}, \textbf{ not } l_{m+2}, \dots, \textbf{ not } l_n
    \end{equation}
    where $h_1, h_2, \dots, h_n$ and $l_1, l_2, \dots, l_n$ are ground literals.
    $\textbf{not}$ refers to default negation~\citep{balduccini_asp_2011}.
\end{definition}

\begin{definition}
    A \textit{constraint} is a rule where $\{ h_1, h_2, \dots, h_n \} = \emptyset$~\citep{balduccini_asp_2011}.
\end{definition}

\begin{definition}
    ``A program is a pair $<\Sigma, \Pi>$ where $\Sigma$ is a signature and $\Pi$ is a set of rules over $\Sigma$~\citep{balduccini_asp_2011}''
\end{definition}

\begin{definition}
    An \textit{answer set} or \textit{model} of a program $\Pi$ is a collection of the consequences of $\Pi$ under ASP semantics~\citep{balduccini_asp_2011}.
    More formally,

    \textbf{Case 1:} Program $\Pi$ consists only of rules without default negation~\citep{blount_architecture_2013}, e.g. those of the form:
    \begin{equation}
        h_1 \lor h_2 \lor \dots \lor h_k \leftarrow l_1, l_2, \dots, l_m
    \end{equation}

    An answer set of $\Pi$ is a consistent set S of ground literals such that~\citep{blount_architecture_2013}:
    \begin{itemize}
        \item ``$S$ satisfies the rules of $\Pi$''~\citep{blount_architecture_2013}.
        \item ``$S$ is minimal, i.e. there is no proper subset of $S$ which satisfies the rules of $\Pi$''~\citep{blount_architecture_2013}.
    \end{itemize}

    \textbf{Case 2:} $\Pi$ is an arbitrary program.
    Let $S$ is a set of ground literals.
    Let $\Pi^S$ be a derivative program of $\Pi$ (called the \textit{reduct} of $\Pi$ with respect to $S$) such that:
    \begin{itemize}
        \item For all literals $l \in S$, remove all rules that contain $\textbf{not } l$
        \item Remove all other clauses containing $\textbf{not}$
    \end{itemize}
    $S$ is an answer set of $\Pi$ if $S$ is an answer set of $\Pi^S$ (see Case 1)~\citep{blount_architecture_2013}.
\end{definition}

$\Pi$ can have multiple answer sets.
For example, the following program (borrowed from \citet{balduccini_asp_2011}):
\begin{gather}
    p \leftarrow \textbf{ not } q. \\
    q \leftarrow \textbf{ not } p.
\end{gather}
has two answer sets: $\{p\}$ and $\{q\}$.

In the case when the condition of a constraint is satisfied, $\Pi$ will have zero answer sets (and will be called \textit{inconsistent} or \textit{unsatisfiable}).
For example, the following program:
\begin{gather}
    p. \\
    \leftarrow p. \label{eq:asp_constraint_example}
\end{gather}
is inconsistent since the literals of constraint \ref{eq:asp_constraint_example} are satisfied (i.e. $p$ is true).

In order to simplify the expression of alternatives for which multiple answer sets can be created, ASP also supports choice rules~\citep{calimeri_asp-core-2_2020,balduccini_asp_2011}.

\begin{definition}
    A \textit{choice rule} (or \textit{constraint literal}) is an expression of the form~\citep{calimeri_asp-core-2_2020,balduccini_asp_2011}:
    \begin{equation}
        m \le \{l_1,l_2, \dots, l_k\} \le n
    \end{equation}
    or
    \begin{equation}
        m \{l_1,l_2, \dots, l_k\} n
    \end{equation}
    where $ l_1,l_2, \dots, l_k $ are literals and $m$, $n$ are arithmetic expressions.
    This expression is satisfied when $m \le |\{ l_1,l_2, \dots, l_k\}| \le n$.

    To simplify this syntax even further, one can also use variables to simplify a constraint literal such as~\citep{balduccini_asp_2011}:
    \begin{equation}
        m \le \{l(a_1, b_2),l(a_2,b_2), \dots, l(a_k, b_k)\} \le n
    \end{equation}
    into an expression of the form:
    \begin{equation}
        m \le \{l(A, B) : dom_a(A), dom_b(B)\} \le n
    \end{equation}
    where $dom_a$ and $dom_b$ are function symbols that define the domain of variables $A$ and $B$, respectively.
\end{definition}

With the \textsc{clingo} grounder/solver\footnotemark, arithmetic expressions can also be expanded to evaluate the value of external functions~\citep{gebser_potassco_2019}.
\footnotetext{
    \textsc{clingo}~\citep{gebser_potassco_2011} is available for download at \url{https://potassco.org/clingo/}
}
\textsc{clingo} allows external functions to be written in Python, Lua, C, Rust, or Haskell, though the latter two have not yet reached a stable API~\citep{roland_kaminski_potasscoclingo_2020,sven_thiele_potasscoclingo-rs_2020,paul_ogris_tsahytclingo-haskell_2020}.
External functions can either be included inline or provided externally\footnotemark.
\footnotetext{
    Only Python and Lua are supported inline.
    \textsc{clingo} must be compiled with Python and/or Lua support for inline syntax to be executed.
}

For example, the program:
\lstinputlisting[language=Prolog]{Figures/ASP/inline_function.lp}
is equivalent to:
\lstinputlisting[language=Prolog]{Figures/ASP/base_equivalent.lp}

While the above example is very trivial, it demonstrates a very powerful technique in which computations or queries can be delegated to functions in traditional programming languages.
This being said, external functions should not be used to achieve side-effects.
Unlike Prolog, \textsc{clingo} evaluates rules and calls external functions nondeterminstically according to internal heuristics.
As such, a developer using \textsc{clingo} has little control over the values with which an external function is called.
A developer can only define domain for variable values, by inserting or removing facts and production rules.

\section{Related Work}

A survey of the literature shows that there have been many other attempts to enable agents to reason over various kinds of policies.
Most involve reasoning over access control policies (e.g. \citet{ferraiolo_role-based_1995,alves_graph-based_2017,barker_logical_2012,sabri_temporal_2016}) and a few utilize ASP as a reasoning tool (e.g. \citet{barker_reasoning_2014,barker_logical_2012}).
Access control policies are more restrictive than the kinds of policies an agent using $\mathcal{AOPL}$ can reason over.
Hence, the work by \citet{liao_extended_2006} is the most relevant for discussion in this thesis.

\subsection{PDC-agent}

\citet{liao_extended_2006} present an extension of the BDI architecture call PDC-agent, which is based off a policy and contract-aware methodology they call BGI-PDC logic\footnotemark.
PDC-agent~\citep{liao_extended_2006} is an event-driven multi-component framework which allows for controlled and coordinated behavior among independent cooperative agents.
\citet{liao_model_2005} use policies to control agent behavior and contracts as a mechanism to coordinate actions between agents.
\citet{peng_extended_2008} extend the PDC-agent architecture to support reasoning over social norms (called NPDC-agent).
However, the discussion of NPDC-agent is not relevant to this section.

\footnotetext{
    \citet{liao_extended_2006} do not mention what these acronyms stand for.
}

\begin{definition}
    The PDC-agent architecture for a single agent is defined as a collection of components, which are grouped into three modules\footnotemark.
    Other cooperative agents are defined likewise.

    \footnotetext{
        The PDC-agent architecture, along with the flow of events between its components, is summarized in \cref{fig:pdc_agent_structure,fig:pdc_agent_control_loop}.
        We have added \cref{fig:pdc_agent_control_loop} based on what \citet{liao_extended_2006} discuss in prose.
    }
\end{definition}

\begin{definition}
    The \textit{Interpreter} module of the PDC-agent architecture is defined as a 7-tuple of the following components: (Event Treating Engine, Belief Update, Contract Engine, Policy Engine, Goal Maintenance, Plan Engine, Plan Library)~\citep{liao_extended_2006}.
    See \cref{appendix:pdc_agent} for an explanation of each component.
\end{definition}

\begin{definition}
    The \textit{State Module} of the PDC-agent architecture consists of the \textit{Intention Set}, \textit{Goal Set}, and \textit{Beliefs}~\citep{liao_extended_2006}.
    Beliefs constitute an agent's understanding of its environment, the agent's internal states, as well as a representation of contracts and policies~\citep{liao_extended_2006}.
\end{definition}

\begin{definition}
    The \textit{Execution Module} of the PDC-agent architecture consists of the \textit{Execution Engine}, \textit{External actions}, and \textit{Internal actions}~\citep{liao_extended_2006}.
\end{definition}

A major distinction of the PDC-agent agent architecture is that the PDC-agent architecture supports coordination among multiple agents.
While both the AAA agent architecture and the $\mathcal{AIA}$ agent architecture can allow multiple agents to operate in a shared environment, the AAA and $\mathcal{AIA}$ architectures do not provide any mechanism to coordinate actions between separate agents so that they can work together towards a common goal.

Similar to the $\mathcal{AIA}$ architecture, PDC-agent distinguishes between external actions and internal actions.
Internal actions in the PDC-agent architecture can update beliefs (analogously to $\mathcal{TI}$) but they can also emit events to the Event Treating Engine.
Internal actions available to agents in the PDC-agent architecture are: AddBelief, DeleteBelief, AddDes (which adds desires to the agent), ExTestObj (which executes ``test objectives''), AddPRE (which adds policy reference events), and AddCooperationInfo (which gathers domain-specific information from cooperating agents)~\citep{liao_extended_2006}.

In spite of these similarities, the PDC-agent architecture has another crucial distinction.
the PDC-agent architecture does not reason over a transition system (or, by extension, an action description) and does not use ASP~\citep{liao_extended_2006}.
Instead, knowledge in PDC-agent is formally represented in terms of a Domain Conceptualization Language (DCL)~\citep{ji_gao_sasa5_2005} and a Concept Instance Pattern (CIP)\footnotemark.
While DCL and CIP can represent plans (which are analogous to activities in the $\mathcal{AIA}$ architecture), there is no support for expressing the direct or indirect effects of an action.
This is a disadvantage in comparison to the action-language-based architectures since plans have to be pre-computed and the goals that they accomplish must be annotated according to intuition of the agent's designer.
Since action languages only require a description of the effects of individual actions (and plans consisting of all permutations of actions can be automatically computed), there is significantly less work for a human designer when working with the AAA architecture or the $\mathcal{AIA}$ architecture than the PDC-agent architecture.

\footnotetext{
    For brevity, this thesis will not include the syntax of DCL and CIP nor will it include a discussion on how these languages are used to formally represent each entity in PDC-agent.
}
