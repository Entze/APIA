\chapter{Background \& Related Work}
\label{ch:background}

% This section is where you will discuss relevant background work, and related works for comparison.
% Ensure that you cite references appropriately, using this as an example~\citep{sample2019}
%
% \section{Background Topic 1}
%
% \section{Background Topic 2}
%
% \section{Related Work}

\section{Intelligent Agents}
\label{sec:agents}

There has been prior research and discussion to determine what constitutes an \textit{agent}.
Though work by \citet{wooldridge_agent_1995} fails to completely define the term,
the authors do propose a set of characteristics that agents must have.
They are summarized by \citet{dignum_intentional_1998} and are as follows.

First, the agent must have autonomy to complete its own actions without the intervention of a human.
This implies that it is making decisions on its own power.
Second, the choices of an agent must be best explained in terms of some form of \textit{intention}.
These intentions can be desires, goals, etc.
An implication of this requirement is that agents cannot be explained through low-level concepts.
Lastly, an agent explained with particular intentions should seem to actually progress their satisfaction.
At the very least, an agent should be \textit{trying} to fulfill its intentions (even if it is doing so poorly).
A system that does not even do this cannot be called an \textit{agent}.

There are many different kinds of agents.
\citet{balke_how_2014} provide a survey of five different kinds of agents.

The first of these are \textit{production rule systems}~\citep{balke_how_2014}.
Production rule systems are symbolic in nature and
consist of three core components:

\begin{itemize}

    \item A set of \textit{rules} (also called \textit{productions}).
        Each rule has the form $ C_i \rightarrow A_i $,
        where $ C_i $ is what is called the \textit{sensory precondition}
        and $ A_i $ is the action to be performed if $ C_i $ is true.
        $ C_i $ is essentially an \textit{if} condition and $ A_i $ is the conditionally executed body.

    \item A knowledge base that stores domain-relevant information about the agent's environment.
        This is called the \textit{working memory}.

    \item A \textit{rule interpreter} that determines which rules apply for the current state of the working memory.
        In the case of rule conflicts, it decides which rule should be executed.

\end{itemize}

% TODO: Consider talking about architecture

Production rule systems are usually implemented in Prolog or Lisp~\citep{balke_how_2014}.

The next class of agents are those ascribing to the \textit{BDI architecture}~\citep{balke_how_2014}.
The belief-desire-intention (BDI) model combines a philosophical model of human practical reasoning
with several successful applications~\citep{georgeff_belief-desire-intention_1999}.
BDI agents are built on the idea of mental states,
consisting of three items: beliefs, desires, and intentions~\citep{balke_how_2014}.
\textit{Beliefs} refer to an agent's representation of the world.
An agent believes something to be true if and only if the statement is found in its representation of the world.
It is worth noting that an agent's beliefs can be inconsistent with its environment (i.e.~the agent can be mistaken).

An agent's \textit{desires} are all possible courses of action that it might want to accomplish in order to reach a goal~\citep{balke_how_2014}.
An agent is not necessarily committed to its desires.
They just play a part in its decision process.
\textit{Intentions}, on the other hand, are commitments to particular courses of action to reach a goal.
Courses of action are also called \textit{plans}.

In addition, BDI agents have a library of plans.
This library consists of pre-computed primitive logic rules to signify which actions contribute to accomplishing which goals.
At each reasoning step, BDI agents search through the plan library to see which plans have a post-condition that matches the currently selected intention
and then sorts them according to relevance.

The third class of agents are those that conform to \textit{normative models}.
They differ from BDI agents in that they are \textit{externally motivated}.
BDI agents are \textit{internally motivated} since all major components (beliefs, goals, and intentions) are internal to the agent.
Normative agents are governed by norms, which are imposed by the agent's external environment.

The last two classes of agents are those with cognitive models~\citep{balke_how_2014}.
They can be divided into those that have \textit{simple} cognitive models and those that have \textit{cognitive architectures}.
The simple cognitive models share similarities with the agent classes presented above
whereas the cognitive architectures are heavily influenced by the structure of the human brain.

\section{Evaluation of agent architectures}
\label{sec:agent_evaluation}

Given these five classes of agent architectures, one needs a way of objectively comparing them.
\citet{balke_how_2014} provide five different dimensions on which these architectures can be evaluated.

The first one is the \textit{cognitive dimension}.
This dimension considers what kind of reasoning is able to be performed by an agent.
\citeauthor{balke_how_2014} evaluate architectures on whether agents are \textit{reactive} or \textit{deliberative}.
Reactive agents are those that only respond to stimuli in their environment and typically follow rule-based patterns.
Deliberative agents, on the other hand, actively consider different courses of action and weigh the utility of each.
For example, production rule systems are an example of reactive agents whereas BDI agents are deliberative~\citep{balke_how_2014}.

The second dimension for agent architecture comparisons is the \textit{affective dimension}~\citep{balke_how_2014}.
The affective dimension considers what degree of emotions agents in an architecture are capable of expressing and acting upon.
Research towards this dimension considers how emotions can be triggered, how they influence the decision process, and how changes in decisions affect the system at large.
Most models are incapable of acting upon emotions~\citep{balke_how_2014} with the exception of two extensions of the BDI model~\citep{jiang_ebdi_2007, dignum_towards_2009} and a cognitive model~\citep{urban_pecs_2000}.

A third dimension for comparing architectures is the \textit{social dimension}.
This dimension considers the extent to which agent architectures provide agents the ability to communicate with other agents, to distinguish social relations (such as status), and to understand complex social concepts.
All the normative models discussed by \citet{balke_how_2014}, along with a few others among BDI extensions and cognitive models, have a notion of self, others, and groups.
BDI agents, in the original model, as well as production rule systems have no notion of social concepts or communication.

Fourth, agent architectures can be evaluated according to the extent of formal and social norms that agents can reason over~\citep{balke_how_2014}.

Lastly, architectures can be compared by considering the type of agent learning that the architecture enables for its agents~\citep{balke_how_2014}.
This dimension considers whether agents can improve or expand their knowledge base given environmental observations.
If agents are capable of doing this, it also considers in what ways the knowledge base can be extended (e.g.~whether they adjust the values for pre-existing decision functions or create new decision rules from scratch)~\citep{balke_how_2014}.
Production rule systems and BDI agents (including its variants) do not support any form of learning whereas normative and cognitive model support it to various extents.

A related consideration for agent architectures is its \textit{elaboration tolerance}.
Elaboration tolerance measures the ease of which a knowledge base can be changed to incorporate new facts~\citep{parmar_formalizing_2003}, whether by learning or by a human redesign.
For example, a representation with a high elaboration tolerance might require very little changes to represent additional information whereas a representation with a low elaboration tolerance may need to be completely re-written to address a new scenario.
The concept was first mentioned by \citet{mccarthy_mathematical_1988}, but later formalized by \citet{parmar_formalizing_2003}.

\section{Logic-based intelligent agents}
\label{sec:logic_based_agents}

In addition to those mentioned by \citet{balke_how_2014}, there has been research on various logic-based agent architectures.
These agent architectures are based off the notion of an \textit{action language} which, in turn, are based on \textit{transition systems}.

\subsection{Transition Systems}
\label{subsec:transition_systems}

Transition systems (also called \textit{transition diagrams}~\citep{blount_architecture_2013}) can be thought of as analogous to a directed graph that models a discrete dynamic domain.
Discrete dynamic domains are environments that behave according to the following rules~\citep{blount_architecture_2013}:

\begin{itemize}
    \item The environment updates in discrete time steps.
    \item Actions occur instantaneously at each time step and the effects appear immediately at the next time step.
    \item Each domain-relevant property is represented by a function.
        The function maps the property name, called a \textit{fluent}, to its value at a particular time step.
        Fluents with their values are collectively referred to as the agent's \textit{state}.
\end{itemize}

Transition systems, then, are akin to directed graphs where the vertices are possible states of the world and edges are actions that, when performed, transition the world from one state to another.
The formal definition is as follows:

\begin{definition}
    \label{def:action_signature}
    An action signature $ < \boldsymbol{V}, \boldsymbol{F}, \boldsymbol{A}> $ consists of three sets: a set $ \boldsymbol{V} $ for value names, a set $ \boldsymbol{F} $ of fluent names, and a set $ \boldsymbol{A} $ of action names~\citep{gelfond_action_1998}.
\end{definition}

\begin{definition}
    \label{def:propositional_action_signature}
    An action signature $< \boldsymbol{V}, \boldsymbol{F}, \boldsymbol{A}>$ is \textit{propositional} if $\boldsymbol{V}=\{t,f\}$~\citep{gelfond_action_1998}.
\end{definition}

\begin{definition}
    \label{def:transition_system}
    A transition system of an action signature $ < \boldsymbol{V}, \boldsymbol{F}, \boldsymbol{A}> $ consists of a set $ S $ of states, a function $ V : \boldsymbol{F} \times S\rightarrow \boldsymbol{V} $, and $ R \subseteq S \times \mathcal{P}(\boldsymbol{A}) \times S $.
\end{definition}

$ V(P,s) $ is said to be the value of fluent $ P $ in state $ s $.
The states $ s' $ such that $ <s, A, s'> \in R $ are the possible results of an actions' $ A $ execution in state $ s $.

\begin{definition}
    \label{def:action_a_determinism}
    $ A $ is \textit{deterministic} in $ s $ if there is exactly one $ s' $~\citep{gelfond_action_1998}.
    If $ A $ is \textit{non-deterministic}, then each $ s' $ \textit{may} be the result of $ A $~\citep{blount_architecture_2013}.
    $ A $ is \textit{executable} in $ s $ iff there is at least one state $ s' $ where $ <s, A, s'> \in R $.
    When $ \left|A\right|>1 $, $A$ represents the execution of concurrent actions~\citep{gelfond_action_1998, blount_architecture_2013}.
\end{definition}

\begin{definition}
    \label{def:wait}
    When $A = \emptyset$, $A$ is sometimes denoted as $Wait$~\citep{gelfond_action_1998}.
\end{definition}

\begin{definition}
    \label{def:event}
    The pair $<s, A>$ is sometimes called an \textit{event}~\citep{gelfond_authorization_2008}.
\end{definition}

\begin{definition}
    \label{def:trajectory}
    A path $ <s_0, A_0, s_1, A_1, \dots, A_{\left\{n-1\right\}}, s_n> $ represents a possible \textit{trajectory} of a system with the initial state $ s_0 $ and final state $ s_n $~\citep{blount_architecture_2013}.
\end{definition}

\begin{definition}
    \label{def:propositional_transition_system}
    If the action signature of a transition system is propositional, then the transition system is said to be \textit{propositional}~\citep{gelfond_action_1998}.
\end{definition}

\subsection{Action languages}
\label{subsec:action_languages}

Action languages are formal models that describe the effects of actions~\citep{gelfond_action_1998}.
There are two kinds of action languages: action description languages and action query languages.
Action description languages describe action effects in terms of a transition system and action query languages state assertions about a transition system~\citep{gelfond_action_1998}.
We will focus only on action description languages and will use the term \textit{action languages} to refer to them.

Many action languages exist, including $ \mathcal{A} $~\citep{gelfond_action_1998}, $ \mathcal{B} $~\citep{gelfond_action_1998}, $ \mathcal{C} $~\citep{gelfond_action_1998}, $ \mathcal{AC} $~\citep{turner_representing_1997}, $ \mathcal{AL} $~\citep{baral_reasoning_2000}, $ \mathcal{ALM} $~\citep{inclezan_modular_2016}.

\subsubsection{Action language $ \mathcal{A} $}
\label{subsubsec:action_language_a}

First, we will discuss action language $ \mathcal{A} $.
Action language $ \mathcal{A} $ was first proposed by \citet{pednault_formulating_1987}.
$ \mathcal{A} $ models a transition system with the signature $ <\{t, f\}, \boldsymbol{F}, \boldsymbol{A}> $.
It is composed of propositions of the form:
\[
a \textbf{ causes } L \textbf{ if } F
\]
where $ a \in \boldsymbol{A} $ is an action name, $ L $ is a literal (also known as the head), and $ F $ is a possibly empty conjunction of literals~\citep{gelfond_action_1998}.
A literal is an element of $(\boldsymbol{F} \cup \{f \in \boldsymbol{F} | \neg f\})$.
Since we model a propositional transition system, $L$ denotes $V(L, s') = t$ for a transition from $s$ to $s'$ where $F$ holds in $s$ and $a$ occurs.
Likewise, $\neg L$ denotes $V(L, s')=f$.
In the case $ F $ is empty, it is denoted as $ True $ and can be omitted (along with the preceding ``if'').
A set of these propositions is called an \textit{action description}~\citep{gelfond_action_1998}.

\begin{definition}
    \label{def:interpretation}
    A function $G$ is an \textit{interpretation} of a set $S$ iff $G: S \rightarrow \{t, f\}$~\citep{gelfond_action_1998}.
\end{definition}

\begin{definition}
    Let $ D $ be an action description in $ \mathcal{A} $.
    The transition system $ <S, V, R> $ described by $ D $ is defined as follows~\citep{gelfond_action_1998}:

    \begin{itemize}
        \item $ S $ is the set of all interpretations of $ \boldsymbol{F} $.
        \item $ V(P, s) $ is the value of fluent $P$ in state $s$.
        \item R is the set of all triples $ <s, \{a\}, s'> $ such that $ E(a,s) \subseteq s' \subseteq E(a,s) \cup s $.
    \end{itemize}
\end{definition}

$ E(a, s) $ is the set of heads $ L $ of all propositions $ a \textbf{ causes } L \textbf{ if } F $ in $ D $ such that $ s $ satisfies $ F $.
$ E(a, s) $ represents all effects of an action $ a $ in state $ s $~\citep{gelfond_action_1998}.
Writing $ E\left(a,s\right)\subseteq s' $ constrains $ s' $ such that it must represent states in which the effects of $ a $ are actually present.
Adding $ \subseteq E\left(a,s\right)\cup s $, allows $ s' $ to equal $ s $.
It is worth noting that there can only be at most one $ s' $ satisfying the above equation~\citep{gelfond_action_1998}.
Therefore, $ a $ is deterministic for every state.

Suppose we had the following action description in $\mathcal{A}$, as given by \citet{gelfond_action_1998}:
\[
a \textbf{ causes } P \textbf{ if } Q
\]
In this case, the action description language models the transition system shown in figure~\ref{fig:action_language_a_example}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/Action_Language_A/Example}
    \caption{Transition system corresponding to example}
    \label{fig:action_language_a_example}
\end{figure}

\subsubsection{Action language $ \mathcal{B} $}
\label{subsubsec:action_language_b}

Action language $ \mathcal{B} $ is an extension of action language $ \mathcal{A} $ [12] that allows the description of actions with indirect effects~\citep{gelfond_action_1998}.
It does so with the addition of so-called \textit{static laws}~\citep{gelfond_action_1998}.
The authors contrast static laws to \textit{dynamic laws}, which refer to action descriptions in $\mathcal{A}$.

Like $\mathcal{A}$, $\mathcal{B}$ models a transition system with the signature $<\{t, f\},\boldsymbol{F},\boldsymbol{A}>$.
An action description in $\mathcal{B}$ is a set of static and dynamic laws, which are as follows~\citep{gelfond_action_1998}:

\begin{itemize}
    \item A static law is written in the following form:
        \[
        L \textbf{ if } F
        \]

    \item A dynamic law, like in $\mathcal{A}$, is written in the form:
        \[
        a \textbf{ causes } L \textbf{ if } F
        \]
\end{itemize}

\noindent
where $a \in \boldsymbol{A}$ is the action name, $L \in(\boldsymbol{F} \cup\{f \in \boldsymbol{F}| \neg f\})$ is a literal (or \textit{head}) and $F \subseteq(\boldsymbol{F} \cup\{f \in \boldsymbol{F}| \neg f\})$ is a, possibly empty, conjunction of literals.

Similar to $\mathcal{A}$, if $F$ is empty then it is denoted as $True$ and can be omitted.

$\mathcal{B}$ defines a notion of closure for sets of literals.

\begin{definition}
    \label{def:causation}
    In a dynamic law $a \textbf{ causes } L$, $a$ is said to \textit{cause} $L$.
    Adding a static law $L' \textbf{ if } L$, $a$ is said to \textit{indirectly cause} $L'$.
\end{definition}

\begin{definition}
    ``A set $s$ of literals is \textit{closed} under a set $Z$ of static laws if $s$ includes the head $L$ of every static law $L \textbf{ if } F$ in $Z$ such that $s$ satisfies $F$''~\citep{gelfond_action_1998}.
    Informally, a set $s$ of literals is closed under a set of static laws $Z$ if there are relevant static laws in $Z$ that require every literal in $s$.
    A static law is informally relevant if its set of literals $F$ is satisfied.
\end{definition}

\begin{definition}
    ``The set $C_{n_Z}\left(s\right)$ of \textit{consequences} of [a set] $s$ [of literals] under [a set] $Z$ [of static laws] is the smallest set of literals that contains $s$ and is closed under $Z$.~\citep{gelfond_action_1998}''
    Informally, the consequences of s are the collection of all effects of a particular fluent value (both direct and indirect).
\end{definition}

\begin{definition}
    Let $D$ be an action description in $\mathcal{B}$.
    The transition system $< S, V, R >$ described by $D$ is defined as follows~\citep{gelfond_action_1998}:

    \begin{itemize}
        \item $S$ is the set of all interpretations of $\boldsymbol{F}$ that are closed under the static laws of $D$.
        \item $ V(P, s) $ is the value of fluent $P$ in state $s$.
        \item $R$ is the set of all triples $<s, \{a\}, s'>$ such that
            \[
            s'=C_{n_Z}\left(E\left(a,s\right)\cup\left(s \cap s'\right)\right)
            \]
            $ Z $ is the set of all static laws of $D$ and $E(a, s)$, as before, is the set of heads $L$ of all propositions $a \textbf{ causes } L \textbf{ if } F$ in $D$ such that $s$ satisfies $F$.
    \end{itemize}
\end{definition}

\subsubsection{Action language $ \mathcal{AL} $}
\label{subsubsec:action_language_al}

$ \mathcal{AL} $ is based on $ \mathcal{A} $ and $ \mathcal{B} $.
It extends these languages with what the authors later refer to as \textit{defined fluents}.
These are domain-relevant properties under the closed world assumption~\citep{blount_architecture_2013}.
The closed world assumption states that which cannot be proven true must be false~\citep{reiter_closed_1981}.
It contrasts from the open world assumption, which states only that which is proven is true~\citep{reiter_closed_1981}.

An action description in $ \mathcal{AL} $ is a set of propositions written in the form~\citep{baral_reasoning_2000, blount_architecture_2013}:

\begin{itemize}
    \item Dynamic laws (from Action language $ \mathcal{A} $):
        \[
        a \textbf{ causes } L \textbf{ if } F
        \]

    \item Static laws (from Action language $ \mathcal{B} $):
        \[
        L \textbf{ if } F
        \]

    \item Executability constraints:
        \[
        \textbf{ impossible } a_0, a_1, \dots, a_k \textbf{ if } F
        \]
\end{itemize}

Like in $\mathcal{A}$ and $\mathcal{B}$, $a \in \boldsymbol{A}$ is the action name, $L \in(\boldsymbol{F} \cup\{f \in \boldsymbol{F}| \neg f\}) $ is a literal (or \textit{head}) and $F \subseteq(\boldsymbol{F} \cup\{f \in \boldsymbol{F}| \neg f\})$ is a possibly empty conjunction of literals.

Later work by \citet{gelfond_knowledge_2014} proposes classifications for literals: \textit{statics}, \textit{inertial fluents}, and \textit{defined fluents}.
The term \textit{fluent} has a slightly different meaning in this context.
\citet{gelfond_knowledge_2014} use the term \textit{domain property} to refer to the \textit{fluent names} introduced in our discussion of action signatures.
By \textit{fluent}, \citet{gelfond_knowledge_2014} mean the domain properties that might actually change between states.
The authors call domain properties that will not change between states (i.e.~the static law $L$) \textit{statics}.
It is worth noting that the distinction between these fluents and statics is only useful when considering the implementation of a system using action language $\mathcal{AL}$.
Representing statics as a runtime constant can result in performance improvements.

\citet{gelfond_knowledge_2014} further partition the term \textit{fluent} into \textit{defined fluents} and \textit{inertial fluents}.
Inertial fluents are inspired by the law of inertial in that their value persists from state to state unless an action explicitly changes it (either directly or indirectly).
Defined fluents, on the other hand, are defined in terms of other fluents and do not necessarily persist from state to state.
Due to the constraints on their definition, they cannot be directly caused by an action.


\subsection{The AAA architecture}
\label{subsec:aaa_architecture}

The first logic-based agent architecture that we will discuss is the \textit{Autonomous Agent Architecture} (AAA) architecture~\citep{balduccini_aaa_2008}.
The AAA architecture is based on work by Gelfond and collaborators~\citep{baral_reasoning_2000,balduccini_diagnostic_2003, balduccini_answer_2006,balduccini_learning_2007}.
As it is a logic-based approach, it makes certain assumptions about the environment in which an agent constructed in its architecture will be operating.
They must be met in order for the architecture to be applicable.
The assumptions are as follows~\citep{balduccini_aaa_2008}:

\begin{itemize}
    \item The world (the agent along with its environment) can be modeled in a transition system.
    \item The agent is competent in interacting with its environment (e.g.~the agent is capable of making correct observations, remembering them, and can perform actions).
    \item Normally, the agent can observe all relevant exogenous events (i.e.~events that are not an effect of an action the agent performed).
\end{itemize}

Given that these assumptions are met, an agent in the AAA architecture performs the following loop once for every time step of the environment~\citep{balduccini_aaa_2008}:

\begin{enumerate}
    \item Observe the world, explain observations, and update knowledge base.
    \item Select an appropriate goal G.
    \item Find a sequence of sets of concurrent actions $A$ (called a \textit{plan}) that achieve G.
    \item Execute first action in plan and update knowledge base.
    \item Repeat (i.e.~go to step 1).
\end{enumerate}

This loop is known as the \textit{Observe-Think-Act} loop~\citep{balduccini_aaa_2008}.

In step 1, the agent observes the world and updates its knowledge base.
This knowledge base is encoded in the Answer Set Prolog (ASP) language, which allows for the representation of many different forms of knowledge [18] (ASP will be discussed later in section~\ref{subsubsec:asp}).
The agent stores the current state of the environment in this knowledge base as well as a history of previous observations and actions~\citep{balduccini_aaa_2008}.

There are two approaches to describing the effects of an agent's actions.
The first is to use action languages to formally denote the action effects, then to translate them into ASP for executability.
The second is to author them directly in ASP.
\citet{balduccini_aaa_2008} choose to do the latter.

It is worth noting that the agent makes no commitment to plans it found in previous loop iterations.
Each loop iteration searches for a plan irrespective to the plan that was partially executed in the previous iteration.
This makes the Observe-Think-Act loop somewhat greedy in that it chooses the best plan as can be foreseen at every time step.
In more recent work~\citep{blount_towards_2014}, \citeauthor{blount_towards_2014} see this as a limitation and builds the $\mathcal{AIA}$ architecture to address it.

\subsection{The $\mathcal{AIA}$ Architecture}
\label{subsec:aia_architecture}

As stated above, the \textit{Architecture for Intentional Agents} ($\mathcal{AIA}$) is an agent architecture that borrows significantly from the AAA architecture~\citep{blount_towards_2014}.
It extends the AAA architecture by representing the possibility for action failure.
In contrast to the AAA architecture, the agent \textit{attempts} to perform an action during its control loop but may find that it is unable to do so.
In this case, the action is \textit{non-executable}~\citep{blount_towards_2014}.

The $\mathcal{AIA}$ architecture shares a number of similar assumptions with the AAA architecture~\citep{blount_towards_2014}:

\begin{itemize}
    \item The agent is capable of making correct observations.
    \item When the agent attempts to perform an action, the action occurs only when it is executable.
        Otherwise, nothing happens.
    \item The agent remembers its observations of the environment (as well as its prior attempts to take actions).
    \item Normally, the agent observes all relevant exogenous events.
\end{itemize}

However, a few assumptions differ from the AAA architecture:

\begin{itemize}
    \item The agent selects a single goal only when it does not have one and focuses on achieving it.
    \item The world must be modeled in an \textit{intentional system description} of $\mathcal{AL}$.
\end{itemize}

\subsubsection{Intentional system description of $\mathcal{AL}$}
\label{subsubsec:intentional_action_language_al}

An intentional system description of $ \mathcal{AL} $ is an extension of $ \mathcal{AL} $ proposed by \citet{blount_architecture_2013}.
This extension is created to address the limitations of the AAA architecture, in which plans are not persisted across iterations of the Observe-Think-Act loop.
In order to persist them, plans are pre-computed and stored as a set of statics.
In the intentional system description of $\mathcal{AL}$, these pre-computed plans are encapsulated in a construct called an \textit{activity}.
Activities are the set of the following statics~\citep{blount_architecture_2013, blount_towards_2014}:

\begin{gather*}
    activity(M). \\
    component(M, 1, C_1). \\
    component(M, 2, C_2). \\
    \dots \\
    component(M, L, C_L). \\
    length(M, L). \\
    goal(M, G).
\end{gather*}

\noindent
where $M$ is a unique identifier for the activity, $C_1, C_2, \dots, C_L$ are the \nth{1}, \nth{2}, \dots, L\textsuperscript{th} part of the plan, and G is the goal that $C_1,C_2,\dots,C_L$ achieves.
In contrast to the AAA architecture, $C_k$ can either refer to an action or another activity $M'$ by its unique identifier.
In the latter case, $M'$ is a child activity of $M$.
An activity cannot be a child (or descendant) of itself (i.e.~activities must be acyclic).
It is also worth noting that activities permit the execution of actions concurrently.

Since activities are pre-computed, there can be an exponentially large amount of unique activities represented by statics.
This can be detrimental to running an implementation so to avoid this, \citet{blount_architecture_2013} only represents activities that are deemed initially relevant and generates the rest on demand.

The intentional system description of $\mathcal{AL}$ uses fluents to keep track of the agent's progress in the current activity.
These fluents define the agent's \textit{mental state} and are updated through \textit{mental actions}.
Mental actions contrast from \textit{physical actions} in that mental actions only affect the internal state of the agent whereas physical actions affect the agent's environment.
The effects of mental actions are defined in a set of propositions called the Theory of Intentions ($\mathcal{TI}$)~\citep{blount_towards_2014}.

\subsubsection{Theory of Intentions ($\mathcal{TI}$)}
\label{subsubsec:theory_of_intentions}

The Theory of Intentions ($\mathcal{TI}$) is a framework of static and dynamic laws that maintain an agent's mental state.
Elements of the agent's mental state include the currently selected goal $G$, stored in the $active\_goal\left(G\right)$ inertial fluent, the current planned activity, stored in the $status(M, k)$ inertial fluent~\citep{blount_towards_2014}.
When either a goal is selected or an activity is planned, they are said to be intended.

$\mathcal{TI}$ updates the agent's mental states through mental actions.
For example, the action $start(M)$ initiates the agent's intention to execute activity $M$ and $stop(M)$ terminates the agent's intention to execute $M$~\citep{blount_towards_2014}.
Though most actions are executed by the agent itself, some must be executed by the agent's controller.
There are two such actions that are exogenous to the agent.
The exogenous mental action $select(G)$ causes the agent to intend to achieve goal $G$~\citep{blount_towards_2014}.
Likewise, the exogenous mental action $abandon(G)$ causes the agent to cease its intent to achieve goal $G$~\citep{blount_towards_2014}.
These actions are necessarily exogenous because, in general, a goal-driven agent must be told to achieve at least one goal beforehand.

\subsubsection{Domain history}
\label{subsubsec:domain_history}

Previously, we mentioned that a differing aspect of the $\mathcal{AIA}$ architecture is that an agent can attempt an action but fail to do so.
This has significant implications on how the agent's history is represented.
Suppose an agent in the AAA architecture has the following action description (borrowed from~\citep{blount_towards_2014}):

\begin{gather*}
    a_1 \textbf{ causes } f \\
    a_2 \textbf{ causes } g \textbf{ if } f, p \\
    b \textbf{ causes } \neg p
\end{gather*}

Suppose the agent has the following history:
\[
\Gamma=\{obs\left(p,true,0\right),obs\left(f,false,0\right),hpd\left(a_{1,}0\right),hpd\left(a_{2,}1\right),obs\left(g,false,2\right)\}
\]
where $obs(F, V, I)$ denotes that the fluent $F$ was observed to have value $V$ at time step $I$ and $hpd(a, I)$ that action $a$ happened at time step $I$.

This history is inconsistent, since the execution of action $a_2$ should have caused $g$ ($g$ is still false afterwards).
The $\mathcal{AIA}$ architecture avoids this inconsistency by adding two history statements.
When executing an action $a$ at time step $I$, the agent records its attempt to do so with $attempt(a, I)$.
During the $I+1$ iteration of the control loop, the agent records $hpd(a, I)$ if the effects of the previous action are observed, otherwise $\neg hpd(a, I)$.
Thus, an agent in the $\mathcal{AIA}$ architecture will have the following history for the previous example

\begin{multline*}
    \Gamma=\{
        obs\left(p,true,0\right),obs\left(f,false,0\right),attempt(a_1, 0),hpd\left(a_{1,}0\right) \\
        attempt(a_2, 1), \neg hpd\left(a_{2,}1\right),obs\left(g,false,2\right)
    \}
\end{multline*}

This history is consistent.

\subsubsection{Control loop}
\label{subsubsec:control_loop}

An agent in the $\mathcal{AIA}$ architecture performs the following loop:

\begin{enumerate}
    \item Interpret observations.
    \item Find intended actions $A$.
    \item Attempt to perform $A$;
        record this attempt in history.
    \item Observe the world;
        record observations in history.
    \item Repeat (i.e.~go to step 1).
\end{enumerate}

\subsubsection{Examples}
\label{subsubsec:aia_examples}

To demonstrate the execution of an agent in the $\mathcal{AIA}$ architecture, we will consider an example given by \citet{blount_architecture_2013}.
Suppose that we have an agent named Bob in a workspace with a colleague named John.
The workspace in which they are located has four consecutive rooms r1, r2, r3, r4 with a door connecting each adjacent room.
Both Bob and John are free to \textit{move} from their current room to an adjoined one.
The only exception is that the door joining r3 and r4 has a lock and must be in an unlocked state in order for anything to pass through it.
Both Bob and John can \textit{lock} and \textit{unlock} the door when next to it.
A visual description of this example is shown in figure~\ref{fig:aia_example}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/AIA_Architecture/Example_1}
    \caption{Visual depiction of $\mathcal{AIA}$ example}
    \label{fig:aia_example}
\end{figure}

\subsubsection{Scenario 1: Planning to achieve goal~\citep{blount_towards_2014}}
\label{subsubsec:aia_scenario_1}

Initially, Bob knows that he is currently located in r1 and John is located in r3.
Suppose Bob is given an order to meet with John.
This is represented as the occurrence of an exogenous mental action $select(meet(Bob, John))$, which eventually causes the $active\_goal(meet(Bob, John))$ fluent to be true in the agent's mental state.
In the first iteration of the $\mathcal{AIA}$ control loop, the agent will:

\begin{enumerate}
    \item Interpret its prior observations, of which there are none.
    \item Find an intended action $a$.
        Since there is not yet an intended goal, the agent will choose to execute the \textit{wait} action, which has no physical or mental effects.
    \item The agent will attempt to execute the \textit{wait} action and record this attempt in its history.
    \item The agent observes the occurrence of the $select(meet(Bob, John))$ exogenous action and record it.
\end{enumerate}

Then, in the second iteration of the loop, the agent will interpret its observations.
According to the $\mathcal{TI}$ laws, this will cause the $active\_goal(meet(Bob, John))$ fluent to be true.
Now, when it will search for an activity which has been described to achieve this goal.
Suppose it chooses the activity $<1, [move(Bob, r1,r2), move(Bob, r2, r3)],meet(Bob,John)>$.
This is represented by the following inertial fluent $status(1,0)$.
The rest of the second iteration is as follows:

\begin{enumerate}
    \setcounter{enumi}{1}
    \item Find intended action $a$.
        This is the k + 1 action of the activity where $status(1, k)$.
        In this case, $a$ is $move(Bob, r1, r2)$.
    \item Execute $move(Bob, r1, r2)$.
        Record $attempt(move(Bob, r1, r2, 1)$ in history.
    \item Observe that Bob is in r2.
        Record $hpd(move(Bob, r1, r2),1)$ in history.
\end{enumerate}

When interpreting its observations in the third iteration of the loop, it will update $status(1,0)$ to $status(1,1)$.
The agent in following steps will intend to execute $move(Bob, r2, r3)$, attempt to execute it and store $attempt(move(Bob, r2, r3))$ in history, and observe that Bob is in r3 and that it has met with John.
Having accomplished its goal, the $active\_goal(meet(Bob, John))$ fluent is set to false and the agent performs the \textit{wait} action until it is given another goal.

\subsubsection{Scenario 2: Serendipitous achievement of goal~\citep{blount_towards_2014}}
\label{subsubsec:aia_scenario_2}

Let us rewind the previous scenario such that the agent Bob is currently attempting $move(Bob, r1, r2)$.
Suppose that, during the same time step, the following exogenous action occurs $move(John, r3, r2)$.
When Bob observes and records the world, it will record $hpd(move(Bob, r1, r2), 1)$ and $hpd(move(John,r3,r2), 1)$.
In the next iteration of the control loop, Bob will interpret his observations and derive the $meet(Bob, John)$ fluent to be true.
In this case, Bob's goal to achieve $meet(Bob, John)$ is serendipitously satisfied.
Since Bob wants to maintain this state, it will intend to perform action $stop(1)$ to halt the execution of activity $1$.
Bob will attempt to perform $stop(1)$ while updating the history.
Bob will observe and record $hpd(stop(1), 2)$, at which point Bob will choose to wait indefinitely.

\subsubsection{Scenario 3: Replanning~\citep{blount_towards_2014}}
\label{subsubsec:aia_scenario_3}

Let us again rewind scenario 1 such that Bob is currently attempting $move(Bob, r1, r2)$.
Suppose that, during the same time step, the following exogenous action occurs $move(John, r3, r4)$.
When Bob interprets this observation, it will recognize that activity $1$ is \textit{futile} (i.e.~it will not satisfy its goal) and so intend to stop activity $1$.
Bob will attempt to perform $stop(1)$, observe that $stop(1)$ has occurred, re-interpret its observations, and search for a new action to perform.
In this step, Bob will look for an activity that satisfies the goal $meet(Bob, John)$.
Suppose it finds the following activity $<2, [move(Bob, r2,r3), move(Bob, r3,r4)],meet(Bob,John)>$.
Then Bob will intend to perform $start(2)$.
After Bob attempts to perform $start(2)$, it will observe that $start(2)$ happened, interpret its observations, then search for an intended action.
Bob will intend to perform $move(Bob, r2,r3)$, attempt to perform it, observe that it happened, interpret its observations, and so on and so forth.
Assuming that $d34$ is unlocked, the scenario will finish like in scenario 1.

\subsubsection{Scenario 4: Abandoning goal~\citep{blount_towards_2014}}
\label{subsubsec:aia_scenario_4}

Moving back to scenario 1 where Bob is currently attempting $move(Bob, r1, r2)$.
Suppose that, during the same time step, the following exogenous action occurs $abandon(meet(Bob, John))$.
Then, when observing this action and interpreting it, the $\mathcal{TI}$ laws will cause $active\_goal(meet(Bob, John))$ fluent to be false.
Subsequently, Bob will intend to stop the current activity (by performing $stop(1)$), attempt to perform $stop(1)$, observe, interpret, and intend to perform \textit{wait} in all future loops.

\subsubsection{Scenario 5: Diagnosis~\citep{blount_towards_2014}}
\label{subsubsec:aia_scenario_5}

Finally, let us reset to the point scenario 1 where Bob is performing $move(Bob, r2, r3)$.
When later observing the world, suppose Bob does not find John there in r3.
After recording this observation in the history, Bob then interprets its observations.
Since activity 1 does not achieve the goal $meet(Bob, John)$, Bob begins diagnostic reasoning over the history to figure out how this could be possible.
Looking back in history, there are three possibilities when John could have moved: when Bob performed $start(1)$, when Bob performed $move(Bob, r1, r2)$, and when Bob performed $move(Bob, r2, r3)$.
Assuming John did not pass Bob while moving between rooms, John must have moved to r4.
In this case, Bob must start an activity $<3, [move(Bob, r3, r4)],meet(Bob,John)>$ to move from r3 to r4.
However, before it can do so, it must first perform stop(1) before performing start(3).
Hence, it takes three more iterations of the control loop to move to r4 ($stop(1)$, $start(3)$, $move(Bob, r3,r4)$) and one final iteration to end the activity ($stop(3)$).

\section{Logic-based approaches to representing and reasoning over agent policies}
\label{sec:policies}

Real-world applications may require agents to behave in particular ways, though they are granted autonomy.
For example, a controller may want to require an agent to follow certain ethical constraints and may choose to penalize an agent when acting in violation of them.
Thus, it is necessary to introduce a discussion on policies for agent behavior and a formalism with which agents can deduce the compliance of their actions.

\subsection{Language $\mathcal{APL}$}

\citet{gelfond_authorization_2008} introduce language $\mathcal{APL}$ to represent authorization policies.
An authorization policy is a set of conditions that denote whether an agent's action is permitted or not permitted~\citep{gelfond_authorization_2008}.
In contrast to the languages discussed previously, $\mathcal{APL}$ is not an action language in and of itself.
$\mathcal{APL}$ only describes the conditions under which actions are permitted or forbidden.
It works in conjunction with action languages such as $\mathcal{AL}$.

\subsubsection{Definition}

Similar to the action languages, $\mathcal{APL}$ requires that the agent's environment be modeled by a transition system $T=<\boldsymbol{V},\boldsymbol{F},\boldsymbol{A}>$.
Recall that $T$ contains all trajectories of the underlying system.
An agent's policy $P$ is the subset of the trajectories of $T$ that are desired by the agent's controller.

\citet{gelfond_authorization_2008} provide a mechanism for describing a policy $P$ by means of \textit{authorization} policy statements.
Thus, a policy $P$ is the set of these statements.
These policies are transcribed using static laws in a similar form to action language $\mathcal{AL}$.

\begin{definition}
    \label{def:authorization_statements}
    Authorization policy statements are static laws of the form~\citep{gelfond_authorization_2008}:

    \begin{gather*}
        permitted\left(a\right) \textbf{ if } F \\
        \neg permitted\left(a\right) \textbf{ if } F \\
        \textbf{normally } permitted(a) \textbf{ if } F \\
        \textbf{normally } \neg permitted(a) \textbf{ if } F
    \end{gather*}

    \noindent
    where
    $s \in S$
    and
    \begin{multline*}
        F\subseteq\left(\{(f, y) \in \boldsymbol{F} \times \boldsymbol{V} | V(f,s)=y\}\right) \cup (\{permitted(a),
        \neg permitted(a), \\
        \textbf{normally } permitted(a),
        \textbf{normally } \neg permitted(a)\}
    \end{multline*}
\end{definition}

\begin{definition}
    \label{def:permission}
    \label{def:denial}
    ~

    \begin{itemize}
        \item A \textit{permission} is a statement of the form $permitted(a)$~\citep{gelfond_authorization_2008}.
        \item A \textit{denial} is a statement of the form $\neg permitted(a)$~\citep{gelfond_authorization_2008}.
    \end{itemize}
\end{definition}

\begin{definition}
    \label{def:strict_authorization_statements}
    A \textit{strict} authorization policy statement is a statement of the form~\citep{gelfond_authorization_2008}:
    \begin{gather*}
        permitted\left(a\right) \textbf{ if } F \\
        \neg permitted\left(a\right) \textbf{ if } F
    \end{gather*}
\end{definition}

\begin{definition}
    \label{def:defeasible_authorization_statements}
    A \textit{defeasible} authorization policy statement is a statement of the form~\citep{gelfond_authorization_2008}:
    \begin{gather*}
        \textbf{normally } permitted(a) \textbf{ if } F \\
        \textbf{normally } \neg permitted(a) \textbf{ if } F
    \end{gather*}
\end{definition}

Defeasible authorization policy statements permit exceptions when they are in conflict with other authorization statements of higher importance.
To encode the relative hierarchy of defeasible statements, \citet{gelfond_authorization_2008} provide an option to label defeasible statements with terms $d_1,d_2,\ldots,d_n$ and use the static $prefer\left(d_i,d_j\right)$ to prioritize statement $d_i$ over statement $d_j$.
Thus, defeasible authorization policy statements can be written in the following form~\citep{gelfond_authorization_2008}.
\begin{gather*}
    d_i: \textbf{normally } permitted(a) \textbf{ if } F \\
    d_j: \textbf{normally } \neg permitted(a) \textbf{ if } F \\
    prefer(d_i, d_j)
\end{gather*}
If a defeasible authorization policy statement is not referenced in a $prefer$ static, then its corresponding label is optional.

\subsubsection{Semantics}

To explain the semantics of $\mathcal{APL}$, we will borrow an example policy from \citet{gelfond_authorization_2008}.
Suppose in a military context, military personnel are given the following rules:

\begin{itemize}
    \item A military officer is not allowed to command a mission he authorized.
    \item A colonel is allowed to command a mission he authorized.
    \item A military observer is never allowed to authorize a mission.
\end{itemize}

For reference, information on military ranking is provided below:

\begin{itemize}
    \item A colonel is a high-ranking military officer.
    \item An officer of any rank outranks an observer.
\end{itemize}

Suppose we have a transition system $T=<\boldsymbol{V}=\{t,f\},\boldsymbol{F},\boldsymbol{A}>$, where
\begin{gather*}
    \boldsymbol{A}=\{authorize(C,M),\ assume\_command(C, M)\} \\
    \boldsymbol{F}=\{authorized(C,M),\ commands(C,M),\ colonel(C),\ observer(C)\}
\end{gather*}
and $M$ and $C$ are variables that range over all missions and commanders, respectively.

As a policy designer, we might interpret the first statement as defeasible as there might be a rare scenario where an officer might be forced to command his own mission.
So, we might write:
\[
\textbf{normally } \neg permitted(assume\_command(C, M)) \textbf{ if } authorized(C, M)
\]
\citet{gelfond_authorization_2008} interpret the second statement as defeasible as well.
Thus, we could write:
\[
\textbf{normally } permitted(assume\_command(C, M)) \textbf{ if } colonel(C)
\]

It is important to note that our policy is ambiguous at this point.
In the case a colonel authorized a mission and begins to command it, he would be both compliant and non-compliant.
To clarify, we will add labels to the previous statements then prefer honoring the second statement over the first.
\begin{gather*}
    d_1(C, M): \textbf{normally } \neg permitted(assume\_command(C, M)) \textbf{ if } authorized(C, M) \\
    d_2(C, M): \textbf{normally } permitted(assume\_command(C, M)) \textbf{ if } colonel(C) \\
    prefer(d_2(C,M),d_1(C,M))
\end{gather*}

Lastly, the third policy statement can be represented with a strict authorization statement.
\[
\neg permitted(authorize(C, M)) \textbf{ if } observer(C)
\]

In total, our policy $P$ be the set of the above two statement blocks.

\subsubsection{Answer Set Programming}
\label{subsubsec:asp}

To check the compliance of an action with a given policy, \citet{gelfond_authorization_2008} first translate the policy using Answer Set Programming and thereby reduce the problem to finding satisfiable models under stable model semantics.

\textit{Answer Set Programming} (ASP) is a form of declarative programming geared towards NP-hard problems~\citep{vladimir_lifschitz_what_2008}.
It is based on stable model semantics~\citep{gelfond_stable_1988}.
The language itself is called \textit{Answer Set Prolog} (or \textit{AnsProlog*} or \textit{A-Prolog}~\citep{baral_answer_2004}) whereas the paradigm as a whole is called \textit{Answer Set Programming}.

\subsubsection{Checking compliance}

As mentioned previously, \citet{gelfond_authorization_2008} use ASP to check the compliance of an action $A$ at state $s$ with a policy $P$.
To do so, \citet{gelfond_authorization_2008} define the function $lp(P,s)$ to translate $P$ into an executable ASP program as follows:

\begin{gather*}
    lp(P,s) = lp(P)\cup lp(s). \\
    lp(P)=\{statement \in P | lp(statement)\} \\
    lp\left(V(f,s)=y\right) =
        val\left(f,y\right). \\
    lp(permitted(a)) =
        permitted(a). \\
    lp(\neg permitted(a)) =
        \neg permitted(a). \\
    lp(permitted(a) \textbf{ if } F) =
        permitted(a) \leftarrow
            lp(F). \\
    lp(\neg permitted(a) \textbf{ if } F) =
        \neg permitted(a) \leftarrow
            lp(F).
\end{gather*}
\begin{multline*}
    lp(d: \textbf{normally } permitted(a) \textbf{ if } F) = \\
        permitted(a) \leftarrow
            lp(F),
            \textbf{not } ab(d),
            \textbf{not } \neg permitted(a).
\end{multline*}
\begin{multline*}
    lp(d: \textbf{normally } \neg permitted(a) \textbf{ if } F) = \\
        \neg permitted(a) \leftarrow
        lp(F),
        \textbf{not } ab(d),
        \textbf{not } permitted(a).
\end{multline*}
\begin{gather*}
    lp(prefer(d_1, d_2)) =
        ab(d_2) \leftarrow lp(cond_1).
\end{gather*}

\noindent
where $lp(s)$ is the logic encoding of the current state, following an approach like that by \citet{balduccini_aaa_2008}.
If S is a set of atoms, then $lp(S)=\{atom \in S | lp(atom)\}$

\begin{definition}
    \label{def:lp_consistent}
    \label{def:lp_categorical}
    ~

    \begin{itemize}
        \item A logic program is \textit{consistent} if it produces at least one answer set~\citep{gelfond_authorization_2008}.
        \item A logic program is \textit{categorical} if it produces exactly one answer set~\citep{gelfond_authorization_2008}.
    \end{itemize}
\end{definition}

\begin{definition}
    \label{def:entails}
    If a consistent logic program $\Pi$ has a literal $l$ which belongs to every answer set of $\Pi$, $\Pi$ \textit{entails} $l$~\citep{gelfond_authorization_2008}.
\end{definition}

\begin{definition}
    \label{def:authorization_consistent}
    \label{def:authorization_categorical}
    ~

    \begin{itemize}
        \item An authorization policy $P$ for $T$ is \textit{consistent} if for every state $s$ of $T$, logic program $lp(P, s)$ is consistent~\citep{gelfond_authorization_2008}.
        \item An authorization policy $P$ for $T$ is \textit{categorical} if for every state $s$ of $T$, logic program $lp(P, s)$ is categorical~\citep{gelfond_authorization_2008}.
    \end{itemize}
\end{definition}

\begin{definition}
    Let $P$ be a consistent authorization policy for $T$.

    \begin{itemize}
        \item $permitted(a) \in P(s)$ iff a logic program $lp(P, s)$ entails $permitted(a)$~\citep{gelfond_authorization_2008}.
        \item $\neg permitted(a) \in P(s)$ iff a logic program $lp(P, s)$ entails $\neg permitted(a)$~\citep{gelfond_authorization_2008}.
    \end{itemize}
\end{definition}

\begin{definition}
    \label{def:authorization_event_compliance}
    ~

    \begin{itemize}
        \item An event $<s, A>$ is \textit{strongly compliant} with $P$ if $\forall a \in A\ permitted(a) \in P(s)$~\citep{gelfond_authorization_2008}.
        \item An event $<s, A>$ is \textit{weakly compliant} with $P$ if $\forall a \in A\ \neg permitted(a) \not \in P(s)$~\citep{gelfond_authorization_2008}.
        \item An event $<s, A>$ is \textit{non-compliant} with $P$ if $\exists a \in A\ \neg permitted(a) \in P(s)$~\citep{gelfond_authorization_2008}.
    \end{itemize}
\end{definition}

\begin{definition}
    \label{def:authorization_path_compliance}
    A path $<s_0, A_0, s_1, A_1, \dots, s_{n-1}, A_{n-1}, s>$ of $T$ can be similarly categorized into \textit{strongly compliant} or \textit{weakly compliant} if every event $<s_i, A_i>$ along the path for $0 \le i < n$ also has the same classification~\citep{gelfond_authorization_2008}.
    If the path has at least one non-compliant event, then the path is \textit{non-compliant}.
\end{definition}

To demonstrate compliance checking, \citet{gelfond_authorization_2008} will use their working example in the context of particular missions and commanders.
Let $P_m$ be a policy constructed in the form of $P$ for a mission $m_1$ and a colonel $c_1$.
$P_m$ is as follows:
\begin{gather*}
    d_1(c_1, m_1): \textbf{normally } \neg permitted(assume\_command(c_1, m_1)) \textbf{ if } authorized(c_1, m_1) \\
    d_2(c_1, m_1): \textbf{normally } permitted(assume\_command(c_1, m_1)) \textbf{ if } colonel(c_1) \\
    prefer(d_2(c_1,m_1),d_1(c_1,m_1)) \\
    \neg permitted(authorize(c_1, m_1)) \textbf{ if } observer(c_1)
\end{gather*}

The logic translation of $P_m$, $lp(P_m)$ is the following:
\lstinputlisting[language=Prolog]{Figures/APL_Language/01_Initial_Example/Pm.lp}

Suppose the agent's environment can be represented by the following state:
\[
s_0 = \{colonel(c1), authorized(c1,m1), \neg commands(c1,m1), \neg observer(c1) \}
\]
Its logic encoding $lp(s_0)$ would be:
\lstinputlisting[language=Prolog]{Figures/APL_Language/01_Initial_Example/s0.lp}

Then, to check the compliance of actions $A \in \mathcal{P}(\boldsymbol{A})$, one would first execute the logic program in an ASP grounder and solver such as clingo.
Doing so, we can see that the logic program $lp(P_m, s_0)$ entails the following answer set:
\lstinputlisting{Figures/APL_Language/01_Initial_Example/clingo.txt}

Considering actions $A=\{assume\_command(c_1, m_1)\}$, since $a = assume\_command(c_1, m_1)$ is the only element of $A$ and $lp(P_m, s_0)$ entails $permitted(assume\_command(c_1, m_1))$, $A$ is strongly compliant with $P_m$.

For completeness, one might be curious to consider actions such as:
\[
A = \{assume\_command(c_1, m_1), authorize(c_1, m_1)\}
\]
While such an action would be weakly compliant according to $P_m$ (since $permitted(authorize(c_1, m_1))$ is missing but neither $\neg permitted(assume\_command(c_1, m_1))$ or $\neg permitted(authorize(c_1, m_1))$ is present), a designer would likely render this combination of actions invalid by accompanying $P_m$ with an action description in $\mathcal{AL}$.
Such an action description might look like the following:

\begin{gather*}
    authorize(c_1, m_1) \textbf{ causes } authorized(c_1, m_1) \\
    assume\_command(c_1, m_1) \textbf{ causes } commands(c_1, m_1) \\
    \textbf{impossible } authorize(c_1, m_1) \textbf{ if } authorized(c_1, m_1) \\
    \textbf{impossible } assume\_command(c_1, m_1) \textbf{ if } commands(c_1, m_1)
\end{gather*}

Intuitively, if $c_1$ has already authorized $m_1$, he cannot authorize it again.
Likewise, if $c_1$ has already assumed command of $m_1$, he cannot begin to assume command of it again.
Thus, actions $A = \{assume\_command(c_1, m_1), authorize(c_1, m_1)\}$ is impossible and is not a valid candidate for checking policy compliance.
$A = \{authorize(c_1, m_1)\}$ is also impossible.

Considering $A=\emptyset$, $A$ is possible according to the action description.
With respect to $P_m$, $A$ is weakly compliant since neither $permitted(wait)$ or $\neg permitted(wait)$ is entailed by $lp(P_m, s_0)$.

\subsubsection{Checking compliance with complete knowledge of state}

To formalize this approach even further, \citet{gelfond_authorization_2008} provide propositions that decide how compliant an event $<s,a>$ is with an authorization policy $P$.

\begin{definition}
    \label{def:authorization_event_compliance_full_knowledge}
    ~

    \begin{itemize}
        \item An event $<s, a>$ is \textit{strongly compliant} with a consistent policy $P$ for $T$ iff the logic program $lp(P, s) \cup \{ \leftarrow permitted(a). \}$ is inconsistent~\citep{gelfond_authorization_2008}.
        \item An event $<s, a>$ is \textit{weakly compliant} with a consistent policy $P$ for $T$ iff the logic program $lp(P, s) \cup \{ \leftarrow \neg permitted(a). \}$ is consistent~\citep{gelfond_authorization_2008}.
        \item An event $<s, a>$ is \textit{non-compliant} with a consistent policy $P$ for $T$ iff the logic program $lp(P, s) \cup \{ \leftarrow \neg permitted(a). \}$ is inconsistent~\citep{gelfond_authorization_2008}.
    \end{itemize}
\end{definition}

These propositions are just simple extensions of definition~\ref{def:authorization_event_compliance}.
However, they are useful because they cause an ASP solver to output a Yes/No answer to the problem.
So, continuing the previous example, one would write:
\begin{lstlisting}[language=Prolog]
:- permitted(assume_command(c1, m1)).
\end{lstlisting}
This logic program results in:
\lstinputlisting{Figures/APL_Language/02_Full_Knowledge/clingo.txt}
Since the output is ``unsatisfiable'', $lp(P, s) \cup \{ \leftarrow permitted(a). \}$ is inconsistent and $<s_0, A>$ is strongly compliant.

\subsubsection{Checking compliance with partial knowledge of state}

It is worth noting that the above approach requires complete knowledge of the state $s$ (i.e. the value of all fluents in $s$).
\citet{gelfond_authorization_2008} show that checking policy compliance can also be possible when an agent only knows the value of a subset of fluents in $s$.
Let $\boldsymbol{F_{lim}} \subseteq \boldsymbol{F}$ be the subset of fluents for which the agent knows $V(f, s)$ where $f \in \boldsymbol{F_{lim}}$.
Let $V_{lim} = \{ f \in \boldsymbol{F_{lim}} | V(f, s)\}$.

\begin{definition}
    \label{def:simple_knowledge_state}
    In contrast to a proper \textit{state} of an action description (where values are assigned to all fluents, provided they satisfy all static laws), $V_{lim}$ describes a \textit{simple-knowledge state} where values are assigned only to some fluents, provided they satisfy relevant static laws~\citep{gelfond_authorization_2008}.
\end{definition}

Let $\delta(V_{lim}) \in S$ be the set of states that contain $V_{lim}$.

\begin{itemize}
    \item If $\forall s_{possible} \in \delta(V_{lim})$ event $<s_{possible}, a>$ is strongly compliant with $P$, then $a$ is strongly compliant in $s$~\citep{gelfond_authorization_2008}.
    \item If $\forall s_{possible} \in \delta(V_{lim})$ event $<s_{possible}, a>$ is at least weakly compliant with $P$, then $a$ is at least weakly compliant in $s$~\citep{gelfond_authorization_2008}.
    \item If $\forall s_{possible} \in \delta(V_{lim})$ event $<s_{possible}, a>$ is non-compliant with $P$, then $a$ is non-compliant in $s$~\citep{gelfond_authorization_2008}.
    \item If none of the above hold, then $V_{lim}$ does not hold enough information to determine the compliance of $a$ in $s$~\citep{gelfond_authorization_2008}.
\end{itemize}

However, in order to evaluate any of the above statements, an agent needs to first compute $\delta(V_{lim})$.
To do so, \citet{gelfond_authorization_2008} rely on converting an action description in $\mathcal{AL}$ to an ASP logic program and joining it to $lp(P, s)$.
They extend $lp$ to handle static laws from $\mathcal{AL}$ as follows:
\[
lp(V(L,s)=y \textbf{ if } F) = val(l, y) \leftarrow lp(F).
\]

Let $SL$ be the set of all static laws from the action description.

Let $D = \{ (f,y) \in \boldsymbol{F_{lim}} \times \boldsymbol{V} | lp(V(f,s)=y)\}$

\begin{definition}
    Let $V_{lim}$ be a simple-knowledge state from an action description in $\mathcal{AL}$.
    $s \in \delta(V_{lim})$ iff lp(s) is an answer set of $lp(V_{lim}) \cup D \cup lp(SL)$~\citep{gelfond_authorization_2008}.
\end{definition}

\begin{definition}
    \label{def:authorization_event_compliance_partial_knowledge}
    ~

    \begin{itemize}
        \item For all actions $s \in \delta(V_{lim})$, the event $<s, a>$ is \textit{strongly compliant} with $P$ iff the program $lp(P, s) \cup D \cup lp(SL) \cup \{ \leftarrow permitted(a). \}$ is inconsistent~\citep{gelfond_authorization_2008}.
        \item For all actions $s \in \delta(V_{lim})$, the event $<s, a>$ is \textit{weakly compliant} with $P$ if $P$ is categorical and the program $lp(P, s) \cup D \cup lp(SL) \cup \{ \leftarrow \textbf{ not } \neg permitted(a). \}$ is inconsistent~\citep{gelfond_authorization_2008}.
        \item For all actions $s \in \delta(V_{lim})$, the event $<s, a>$ is \textit{non-compliant} with $P$ iff the program $lp(P, s) \cup D \cup lp(SL) \cup \{ \leftarrow \neg permitted(a). \}$ is inconsistent~\citep{gelfond_authorization_2008}.
        \item If $P$ is categorical, then an event $<s, a>$, where $s \in \delta(V_{lim})$, is also non-compliant with $P$ if the program $lp(P,s) \cup D \cup lp(SL) \cup \{ \leftarrow \textbf{ not } \neg permitted(a). \}$ is consistent~\citep{gelfond_authorization_2008}.
    \end{itemize}
\end{definition}

%\lstinputlisting[language=Prolog]{Figures/APL_Language/03_Partial_Knowledge/Pm.lp}
\lstinputlisting[language=Prolog]{Figures/APL_Language/03_Partial_Knowledge/s0_lim.lp}
\lstinputlisting[language=Prolog]{Figures/APL_Language/03_Partial_Knowledge/D.lp}
\lstinputlisting[language=Prolog]{Figures/APL_Language/03_Partial_Knowledge/SL.lp}

\lstinputlisting{Figures/APL_Language/03_Partial_Knowledge/clingo.txt}

\subsubsection{Checking compliance with minimal knowledge of state}

\citet{gelfond_authorization_2008} note that the above two approaches will not work for agents in the AAA architecture (or for that matter, the $\mathcal{AIA}$ architecture).
Nonetheless, they develop a method for checking policy compliance under the AAA architecture knowledge constraints.
As mentioned previously, agents in the AAA architecture have an initial description of their initial state (complete or partial), a history of their actions $A_0, A_1, \dots, A_n$, and may have memory of fluent values at particular points in time.

\citet{gelfond_authorization_2008} define a new function $lp(P, I)$ that adds a step $I$ argument to each ASP atom from $lp(P)$.
For example,
\[
lp(V(f,s)=y, I) = val(f,y,I)
\]

For a history $H_n=<s, a_0, a_1, \dots, a_{n-1}>$, they also add:
\begin{gather*}
    lp(a, I)=occurs(a, I). \\
    lp(P, H) = lp(P, I) \cup lp(s, 0) \cup lp(a_0, 0) \cup lp(a_1, 1) \cup \dots \cup lp(a_{i-1}, I-1)
\end{gather*}

Let $D_0 = \{ (f,y) \in \boldsymbol{F} \times \boldsymbol{V} | lp(V(f,s)=y, 0)\}$

\begin{definition}
    \label{def:model_of_history}
    A trajectory $s_0, a_0, s_1, a_1, \dots, a_{n-1}, s_n$ of a transition system $T$ is a \textit{model} of a history $H_n = <s, a_0, a_1, \dots, a_{n-1}>$ if $s \subseteq s_0$~\citep{gelfond_authorization_2008}.
    Thus, a trajectory that meets this condition does not contradict what has been recorded in the agent's history.
\end{definition}

Let $Check_1(H_n)$ be the following subprogram:
\begin{gather*}
    \neg strongly\_compliant \leftarrow occurs(A, I), \textbf{ not } permitted(A, I). \\
    \leftarrow \textbf{ not } \neg strongly\_compliant.
\end{gather*}

Let $Check_2(H_n)$ be the following subprogram:
\begin{gather*}
    \neg weakly\_compliant \leftarrow occurs(A, I), \neg permitted(A, I). \\
    \leftarrow \textbf{ not } \neg weakly\_compliant.
\end{gather*}

\begin{definition}
    \label{def:authorization_event_compliance_history_knowledge}
    ~

    \begin{itemize}
        \item A model of history $H_n$ is \textit{strongly compliant} with $P$ iff the program $lp(P, H_n) \cup D \cup lp(A) \cup Check_1$ is inconsistent~\citep{gelfond_authorization_2008}.
        \item A model of history $H_n$ is \textit{weakly compliant} with $P$ iff the program $lp(P, H_n) \cup D \cup lp(A) \cup Check_2$ is inconsistent~\citep{gelfond_authorization_2008}.
        \item A model of history that is neither strongly compliant nor weakly compliant must be non-compliant~\citep{gelfond_authorization_2008}.
    \end{itemize}
\end{definition}

%\lstinputlisting[language=Prolog]{Figures/APL_Language/04_Minimal_Knowledge/P.lp}
\lstinputlisting[language=Prolog]{Figures/APL_Language/04_Minimal_Knowledge/Hn.lp}
%\lstinputlisting[language=Prolog]{Figures/APL_Language/04_Minimal_Knowledge/D.lp}
\lstinputlisting[language=Prolog]{Figures/APL_Language/04_Minimal_Knowledge/A.lp}
\lstinputlisting[language=Prolog]{Figures/APL_Language/04_Minimal_Knowledge/Check_1.lp}
\lstinputlisting[language=Prolog]{Figures/APL_Language/04_Minimal_Knowledge/Check_2.lp}

\lstinputlisting{Figures/APL_Language/04_Minimal_Knowledge/clingo.txt}

\subsection{Language $\mathcal{AOPL}$}

In addition to introducing language $\mathcal{APL}$, \citet{gelfond_authorization_2008} introduce language $\mathcal{AOPL}$ to consider ``obligation policies'' in addition to authorization policies.
Authorization policies describe what an agent can do whereas obligation policies describe what an agent cannot do.

\subsubsection{Definition}

Language $\mathcal{AOPL}$ uses $\mathcal{APL}$ as its basis and shares its underlying assumptions, such as requiring the agent's environment be modeled by a transition system $T=<\boldsymbol{V}, \boldsymbol{F}, \boldsymbol{A}>$ and that a policy $P$ is a subset of the trajectories of $T$ that are preferable to the agent's controller.

$\mathcal{AOPL}$ adds a $obl(a)$ statement in contrast to $\mathcal{APL}$.
If $obl(a)$ is true, the policy $P$ dictates that the agent must execute $a$ in the current state.
If $obl(\neg a)$ is true, then $P$ dictates that the agent must not execute $a$ in the current state.
Obligation policy statements are static laws of the form~\citep{gelfond_authorization_2008}:
\begin{gather*}
    obl\left(a\right) \textbf{ if } F \\
    \neg obl\left(a\right) \textbf{ if } F \\
    d_i: \textbf{normally } obl(a) \textbf{ if } F \\
    d_j: \textbf{normally } \neg obl(a) \textbf{ if } F \\
    prefer(d_i, d_j)
\end{gather*}
where
\begin{multline*}
    F\subseteq\left(\{(f, y) \in \boldsymbol{F} \times \boldsymbol{V} | V(f,s)=y\}\right) \cup (\{obl(a),
        \neg obl(a), \\
        \textbf{normally } obl(a),
        \textbf{normally } \neg obl(a)\}
\end{multline*}
and $a \in \boldsymbol{A} \cup \{a' \in \boldsymbol{A} | \neg a'\}$.

\subsubsection{Checking compliance}

To evaluate the compliance of action $a$ at state $s$ with an obligation policy $P$, \citet{gelfond_authorization_2008} extend $lp$ as follows:

\begin{gather*}
    lp(obl(a)) =
        obl(a). \\
    lp(\neg obl(a)) =
        \neg obl(a). \\
    lp(obl(a) \textbf{ if } F) =
        obl(a) \leftarrow
            lp(F). \\
    lp(\neg obl(a) \textbf{ if } F) =
        \neg obl(a) \leftarrow
            lp(F).
\end{gather*}
\begin{multline*}
    lp(d: \textbf{normally } obl(a) \textbf{ if } F) =
        obl(a) \leftarrow \\
            lp(F),
            \textbf{not } ab(d),
            \textbf{not } \neg obl(a). \\
    lp(d: \textbf{normally } \neg obl(a) \textbf{ if } F) =
        \neg obl(a) \leftarrow \\
            lp(F),
            \textbf{not } ab(d),
            \textbf{not } obl(a).
\end{multline*}
\begin{gather*}
    lp(prefer(d_1, d_2)) =
        ab(d_2) \leftarrow lp(cond_1).
\end{gather*}

\begin{definition}
    $obl(h) \in P(s)$ iff the logic program $lp(P, s)$ entails $obl(h)$
\end{definition}

\begin{definition}
    An event $<s, A>$ is \textit{compliant} with an obligation policy $P$ if $(\forall a \in A \ obl(a) \in P(s)) \land (\forall a \not \in A \ obl(\neg a) \in P(s))$
\end{definition}

This definition of compliance contrasts with that for authorization policies.
Given a set of actions $A$, the definition for authorization policies can be thought of first enumerating action $a \in A$ then checking to see if $permitted(a)$ exists.
The definition for obligation policies, by contrast, can be thought of first searching for $obl(a)$ statements, then checking to see if $a \in A$ exists.

Furthermore, the definition for obligation compliance does not make differentiate between strong compliance and weak compliance.
Were such a differentiation to exist, it would center around instances where neither $\neg obl(a)$ nor $obl(a)$ is true.
For such a case, an obligation policy would not explicitly impose nor explicitly waive an agent from an obligation to perform action $a$.
However, since $\mathcal{AOPL}$ does not consider `weak compliance', this situation is simply called \textit{compliant}.

Let $P$ be an arbitrary policy $P$ written in $\mathcal{AOPL}$.

\begin{definition}
    ~

    \begin{itemize}
        \item Let $P_a$ be a derivative policy of $P$ that only has the authorization rules of $P$.
            $P_a$ is an authorization policy \textit{induced} by $P$~\citep{gelfond_authorization_2008}.
        \item Likewise, let $P_o$ be a derivative of $P$ such that $P_o$ only has the obligation rules of $P$.
            $P_o$ is an obligation policy \textit{induced} by $P$~\citep{gelfond_authorization_2008}.
    \end{itemize}
\end{definition}

\subsubsection{Obligation example}

To demonstrate the compliance of a policy $P$ in $\mathcal{AOPL}$, \citet{gelfond_authorization_2008} provide the following example.

Suppose a university professor were to encode his classroom policies into $\mathcal{AOPL}$.
He might have requirements such as:

\begin{itemize}
    \item Students are expected to not miss class.
    \item Students should submit homework on time.
    \item Students should do their homework independently.
\end{itemize}

Before we encode these obligations, we need to first define the transition system for this scenario.
Suppose the environment can be modeled by a transition system $T=<\boldsymbol{V}=\{t,f\}, \boldsymbol{F}, \boldsymbol{A}>$ where:
\begin{multline*}
    \boldsymbol{F} = \{enrolled(Student, Class), m(Class, SessionNum), \\
        due\_date(SessionNum, Assignment), family\_emergency(Student, SessionNum), \\
        religious\_holiday(M)\}
\end{multline*}
\begin{multline*}
    \boldsymbol{A} = \{attend(Student, Class), submit(Student, Assignment, SessionNum), \\
        accept\_unauthorized\_help(Student)\}
\end{multline*}
and $Student$, $Class$, $Assignment$, and $SessionNum$ are variables that range over their respective values.

Now that we have a transition system defined, we are ready to transcribe the verbal requirements into policy strict/defeasible statements in $\mathcal{AOPL}$.
\citet{gelfond_authorization_2008} envision possible exceptions to the first rule and, in order to maintain elaboration tolerance, express it with a defeasible obligation statement:
\[
d_1(S,C,N): \textbf{ normally } obl(attend(S, meeting(C, N))) \textbf{ if } enrolled(S, C).
\]

\citet{gelfond_authorization_2008} assume that such an implicit exception may be ``Students do not need to attend class during family emergencies.''
Its encoding would be as follows:
\[
\neg obl(attend(S,M)) \textbf{ if } family\_emergency(S,M).
\]

Similarly, they suggest that another exception may be ``Students should not attend class on a religious holiday.''
Its encoding might be as follows:
\[
d_2(S,M): \textbf{ normally } obl(\neg attend(S, M)) \textbf{ if } religious\_holiday(M).
\]

\citet{gelfond_authorization_2008} note that writing this rule as defeasible allows the designer to decide whether religious obligations overrule secular requirements.
If he does, he may write:
\[
prefer(d_2(S, m(C,N)),d_1(S,C,N))
\]
Otherwise, he may write:
\[
prefer(d_1(S,C,N), d_2(S, m(C,N)))
\]

While it is possible to omit such a statement from the policy description, doing so would produce an ambiguous policy that would allow an agent to choose which rule it would prefer to uphold.
\citet{gelfond_authorization_2008} do not recommend allowing this behavior.

The second rule may be seen as defeasible to encompass assignment extensions.
\begin{multline*}
    d_3(S,C,N_1,N_2): \textbf{ normally } obl(submit(S,a(C,N_1),m(C,N_2)) \textbf{ if } enrolled(S,C), \\
        due\_date(m(C,N_2), a(C,N_1)).
\end{multline*}
Lastly, since cheating is never permissible:
\[
obl(\neg accept\_unauthorized\_help(S)).
\]

\subsection{\texorpdfstring{BGI\textsubscript{PDC}}{BGI-PDC}}

\subsection{PDC-agent}
