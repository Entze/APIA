\chapter{Background \& Related Work}
\label{ch:background}

% This section is where you will discuss relevant background work, and related works for comparison.
% Ensure that you cite references appropriately, using this as an example~\cite{sample2019}
%
% \section{Background Topic 1}
%
% \section{Background Topic 2}
%
% \section{Related Work}

\section{Intelligent Agents}
\label{sec:agents}

There has been prior research and discussion to determine what constitutes an ``agent''.
Though work in \cite{wooldridge_agent_1995} fails to completely define the term,
the authors do propose a set of characteristics that agents must have.
They are summarized in \cite{dignum_intentional_1998} and are as follows.

First, the agent must have autonomy to complete its own actions without the intervention of a human.
This implies that it is making decisions on its own power.
Second, the choices of an agent must be best explained in terms of some form of \textit{intention}.
These intentions can be desires, goals, etc.
An implication of this requirement is that agents cannot be explained through low-level concepts.
Lastly, an agent explained with particular intentions should seem to actually progress their satisfaction.
For example, a system with the intention of achieving a goal,
but fails to do so when alternatives would have done so, cannot be called an ``agent''.

There are many different kinds of agents.
\cite{balke_how_2014} provides a survey of five different kinds of agents.

The first of which are production rule systems \cite{balke_how_2014}.
Production rule systems are symbolic in nature and
consist of three core components:

\begin{itemize}

    \item A set of \textit{rules} (also called \textit{productions}).
        Each rule has the form $ C_i \rightarrow A_i $,
        where $ C_i $ is what is called the \textit{sensory precondition}
        and $ A_i $ is the action to be performed if $ C_i $ is true.
        $ C_i $ is essentially an ``if'' condition and $ A_i $ is the conditionally executed body.

    \item A knowledge base that stores domain-relevant information about the agent's environment.
        This is called the \textit{working memory}.

    \item A \textit{rule interpreter} that determines which rules apply for the current state of the working memory.
        In the case of rule conflicts, it decides which rule should be executed.

\end{itemize}

% TODO: Consider talking about architecture

Production rule systems are usually implemented in Prolog or LISP \cite{balke_how_2014}.

The next class of agents are those ascribing to the BDI architecture \cite{balke_how_2014}.
The belief-desire-intention (BDI) model combines a philosophical model of human practical reasoning
with several successful applications \cite{georgeff_belief-desire-intention_1999}.
BDI agents are built on the idea of mental states,
consisting of three items: beliefs, desires, and intentions \cite{balke_how_2014}.
Beliefs refer to an agent's representation of the world.
An agent believes something to be true if and only if the statement is found in its representation of the world.
It is worth noting that an agent's beliefs can be inconsistent with its environment.
In this case, the agent would be wrong.

\cite{rao_modeling_1991}

The desires of an agent are all possible courses of action that it might want to accomplish in order to reach a goal \cite{balke_how_2014}.
An agent is not necessarily committed to its desires.
They just play a part in its decision process.
Intentions, on the other hand, are commitments to particular courses of action to reach a goal.
Courses of action are also called plans.

In addition, BDI agents have a library of plans.
This library consists of pre-computed primitive logic rules to signify which actions contribute to accomplishing which goals.
At each reasoning step, BDI agents search through the plan library to see which plans have a post-condition that matches the currently selected intention
and then sorts them according to relevance.

The third class of agents are those that conform to normative models.
They differ from BDI agents in that they are externally motivated.
BDI agents are internally motivated since all major components (beliefs, goals, and intentions) are internal to the agent.
Normative agents are governed by norms, which are imposed by the agent's external environment.

The last two classes of agents are those with cognitive models \cite{balke_how_2014}.
They can be divided into those that have ``simple'' cognitive models and those that have ``cognitive architectures''.
The simple cognitive models share similarities with the agent classes presented above
whereas the cognitive architectures are heavily influenced by the structure of the human brain.

\section{Evaluation of agent architectures}
\label{sec:agent_evaluation}

Given these five classes of agent architecture, one needs a way of objectively comparing them.
\cite{balke_how_2014} provides five different dimensions on which these architectures can be evaluated.

The first of which is the cognitive dimension.
This dimension considers what kind of reasoning is able to be performed by an agent.
\cite{balke_how_2014} provides two partitions for cognitive ability:
reactive or deliberative and composed of simple cognitive components or of psychologically/neurologically inspired ones.
Reactive agents are those that only respond to stimuli in their environment and typically follow rule-based patterns.
Deliberative agents, on the other hand, actively consider different courses of action and weigh the utility of each.
For example, production rule systems are an example of reactive agents whereas BDI agents can be deliberative \cite{balke_how_2014}.
Agents composed of psychologically/neurologically inspired cognitive components are those whose design strives to model the human brain,
whereas agents composed of simple cognitive components are those whose design consists of manmade abstractions.
Psychologically/neurologically inspired agents correspond to the cognitive-model class mentioned above.

The second dimension for agent architecture comparisons is the affective dimension \cite{balke_how_2014}.
The affective dimension considers what degree of emotions agents in an architecture are capable of expressing and acting upon.
Research towards this dimension considers how emotions can be triggered, how they influence the decision process, and how changes in decisions affect the system at large.
Most models are incapable of acting upon emotions \cite{balke_how_2014} with the exception of two extensions of the BDI model \cite{jiang_ebdi_2007, dignum_towards_2009} and a cognitive model \cite{urban_pecs_2000}.

A third dimension for comparing architectures is the social dimension.
This dimension considers the extent to which agent architectures provide agents the ability to communicate with other agents, to distinguish social relations (such as status), and to understand complex social concepts.
All the normative models discussed in \cite{balke_how_2014}, along with a few others among BDI extensions and cognitive models, have a notion of self, others, and groups.
BDI agents, in the original model, as well as production rule systems have no notion of social concepts or communication.

Fourth, agent architectures can be evaluated according to the extent of formal and social norms that agents can reason over \cite{balke_how_2014}.

Lastly, architectures can be compared by considering the type of agent learning that the architecture enables for its agents \cite{balke_how_2014}.
This dimension considers whether agents can improve or expand their knowledge base given environmental observations.
If agents are capable of doing this, it also considers in what ways the knowledge base can be extended (e.g. whether they adjust the values for pre-existing decision functions or create new decision rules from scratch) \cite{balke_how_2014}.
Production rule systems and BDI agents (including its variants) do not support any form of learning whereas normative and cognitive model support it to various extents.

A related consideration for agent architectures is its elaboration tolerance.
Elaboration tolerance measures the ease of which a knowledge base can be changed to incorporate new facts \cite{parmar_formalizing_2003}, whether by learning or by a human redesign.
For example, a representation with a high elaboration tolerance might require very little changes to represent additional information whereas a representation with a low elaboration tolerance may need to be completely re-written to address a new scenario.
The concept was first mentioned in \cite{mccarthy_mathematical_1988}, but later formalized in \cite{parmar_formalizing_2003}.

\section{Logic-based intelligent agents}
\label{sec:logic_based_agents}

In addition to those mentioned in \cite{balke_how_2014}, there has been research on various logic-based agent architectures.
These agent architectures are based off of the notion of an \textit{action language} which, in turn, are based on \textit{transition systems}.

\subsection{Transition Systems}
\label{subsec:transition_systems}

Transition systems (also called ``transition diagrams'' \cite{blount_architecture_2013}) can be thought of analogous to a directed graph that models a discrete dynamic domain.
Discrete dynamic domains are environments that behave according to the following rules \cite{blount_architecture_2013}:

\begin{itemize}
    \item The environment updates in discrete time steps
    \item Actions occur instantaneously at each time step and the effects appear immediately at the next time step
    \item Each domain-relevant property is represented by a function.
        The value of the function at each time step is the value of the property.
        These properties are known as ``state''.
\end{itemize}

Transition systems, then, are akin to directed graphs where the vertices are possible states of the world and edges are actions that, when performed, transition the world from one state to another.
The formal definition is as follows:

\begin{definition}{Action signature}
    An action signature $ <V,F,A> $ consists of three sets: a set $ V $ for value names, a set $ F $ of fluent names, and a set $ A $ of action names \cite{gelfond_action_1998}.
\end{definition}

\begin{definition}{Transition system}
    A transition system of an action signature $ <V,F,A> $ consists of a set $ S $ of states, a function $ V : F \times S\rightarrow V $, and $ R\ \subseteq S \times A \times S $.
\end{definition}

$ V(P,s) $ is said to be the value of $ P $ in $ s $.
The states $ s' $ such that $ <s, A, s'> $ in $ R $ are the possible results of an action $ A $'s execution in state $ s $.
$ A $ is deterministic in $ s $ if there is exactly one $ s' $ \cite{gelfond_action_1998}.
If $ A $ is non-deterministic, then each $ s' $ may be the result of $ A $ \cite{blount_architecture_2013}.
$ A $ is executable in $ s $ iff there is at least one state $ s' $ where $ <s, A, s'> $ in $ R $.
When $ \left|A\right|>1 $, A represents the execution of concurrent actions \cite{gelfond_action_1998, blount_architecture_2013}.

A path $ <s_0, A_0, s_1, A_1, \ldots, A_{\left\{n-1\right\}}, s_n> $ represents a possible trajectory of a system with the initial state $ s_0 $ and final state $ s_n $ \cite{blount_architecture_2013}.

\subsection{Action languages}
\label{subsec:action_languages}

Action languages are formal models that describe the effects of actions \cite{gelfond_action_1998}.
There are two kinds of action languages: action description languages and action query languages.
Action description languages describe action effects in terms of a transition system and action query languages state assertions about a transition system \cite{gelfond_action_1998}.
We will focus only on action description languages and will use the term ``action languages'' to refer to them.

Many action languages exist, including $ \mathcal{A} $ \cite{gelfond_action_1998}, $ \mathcal{B} $ \cite{gelfond_action_1998}, $ \mathcal{C} $ \cite{gelfond_action_1998}, $ \mathcal{AC} $ \cite{turner_representing_1997}, $ \mathcal{AL} $ \cite{baral_reasoning_2000}, $ \mathcal{ALM} $ \cite{inclezan_modular_2016}.

\subsubsection{Action language $ \mathcal{A} $}
\label{subsubsec:action_language_a}

First, we will discuss action language $ \mathcal{A} $.
Action language $ \mathcal{A} $ was first proposed in \cite{pednault_formulating_1987}.
A models a transition system with the signature $ <{t,\ f},\ F,\ A> $.
It is composed of sets of propositions of the form:

$$
A\ causes\ L\ if\ F
$$

$ A $ is an action name, $ L $ is a literal (also known as the head), and $ F $ is a, possibly empty, conjunction of literals \cite{gelfond_action_1998}.
In the case $ F $ is empty, it is denoted as $ True $ and can be omitted (along with the preceding ``if'').
A set of these propositions is called an action description \cite{gelfond_action_1998}.

\begin{definition}{Action description}
    Let $ D $ be an action description in $ \mathcal{A} $.
    The transition system $ <S,\ V,\ R> $ described by $ D $ is defined as follows \cite{gelfond_action_1998}:

    \begin{itemize}
        \item $ S $ is the set of all interpretations of $ F $
        \item $ V(P, s) = s(P) $
        \item R is the set of all triples $ <s,\ A,\ s'> $ such that $ E(A,s) \subseteq s' \subseteq E(A,s) \cup s $
    \end{itemize}
\end{definition}

$ E(A,\ s) $ is the set of heads $ L $ of all propositions $ A\ causes\ L\ if\ F $ in $ D $ such that $ s $ satisfies $ F $.
$ E(A,\ s) $ represents all effects of an action $ A $ in state $ s $ \cite{gelfond_action_1998}.
Writing $ E\left(A,s\right)\subseteq s' $ constrains $ s' $ such that it must represent states in which the effects of $ A $ are actually present.
Adding $ \subseteq E\left(A,s\right)\cup s $, allows $ s' $ to equal $ s $.
It is worth noting that there can only be at most one $ s' $ satisfying the above equation \cite{gelfond_action_1998}.
Therefore, $ A $ is deterministic for every state.

Suppose we had the following action description in A, as given in \cite{gelfond_action_1998}:

$$
A\ causes\ P\ if\ Q
$$

In this case, the action description language models the transition system shown in figure~\ref{fig:action_language_a_example}.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/Action_Language_A/Example}
    \caption{Transition system corresponding to example}
    \label{fig:action_language_a_example}
\end{figure}

\subsubsection{Action language $ \mathcal{B} $}
\label{subsubsec:action_language_b}

Action language $ \mathcal{B} $ is an extension of Action language $ \mathcal{A} $ [12] that allows the description of actions with indirect effects.
\cite{gelfond_action_1998}.
It does so with the addition of so-called \textit{static laws} \cite{gelfond_action_1998}.
The authors contrast static laws to \textit{dynamic laws}, which refer to action descriptions in A.

Like A, B models a transition system with the signature $<{f,\ t},F,A>$.
An action description in B is a set of static and dynamic laws, which are as follows \cite{gelfond_action_1998}:

\begin{itemize}
    \item A static law is written in the following form:
        $$
        L\ if\ F
        $$

    \item A dynamic law, like in A, is written in the form:
        $$
        A\ causes\ L\ if\ F
        $$
\end{itemize}

$A$ is the action name, $L$ is a literal (or head) and $F$ is a, possibly empty, conjunction of literals.

Similar to $A$, if $F$ is empty then it is denoted as $True$ and can be omitted.

$B$ defines a notion of closure for sets of literals.

\begin{definition}{Closure}
    ``A set s of literals is \textit{closed} under a set Z of static laws if s includes the head L of every static law L\ if\ F in Z such that s satisfies F.
\cite{gelfond_action_1998}''
\end{definition}

\begin{definition}{Consequences}
    ``The set $C_{n_Z}\left(s\right)$ of \textit{consequences} of $s$ under $Z$ is the smallest set of literals that contains $s$ and is closed under $Z$.
    \cite{gelfond_action_1998}''
\end{definition}

\begin{definition}{Action description}

    Let $D$ be an action description in $B$.
    The transition system $<\ S,\ V,\ R\ >$ described by $D$ is defined as follows \cite{gelfond_action_1998}:

    \begin{itemize}
        \item S is the set of all interpretations of F that are closed under the static laws of D
        \item V(P,\ s)=s(P)
        \item R is the set of all triples $<s,\ A,\ s\prime>$ such that
            $$
            s^\prime=C_{n_Z}\left(E\left(A,s\right)\cup\left(s\ \cap\ s^\prime\right)\right)
            $$

            $ Z $ is the set of all static laws of $D$ and $E(A, s)$, as before, is the set of heads $L$ of all propositions $A$ causes $L$ if $F$ in $D$ such that $s$ satisfies $F$.
    \end{itemize}
\end{definition}

\subsubsection{Action language $ \mathcal{AL} $}
\label{subsubsec:action_language_al}

$ \mathcal{AL} $ is based on $ \mathcal{A} $ and $ \mathcal{B} $.
It extends these languages with what it calls ``defined fluents'', which are domain-relevant properties under the closed world assumption \cite{blount_architecture_2013}.
The closed world assumption states: that which cannot be proven true must be false \cite{reiter_closed_1981}.
It contrasts from the open world assumption, which states: only that which is proven is true \cite{reiter_closed_1981}.

An action description in $ \mathcal{AL} $ is a set of propositions written in the form \cite{baral_reasoning_2000, blount_architecture_2013}:

\begin{itemize}
    \item Dynamic laws (from Action language $ \mathcal{A} $):
        $$
        A\ causes\ L\ if\ F
        $$

    \item Static laws (from Action language $ \mathcal{B} $):
        $$
        L\ if\ F
        $$

    \item Executability constraints:
        $$
        impossible\ A\ if\ F
        $$
\end{itemize}

Like in $A$ and $B$, $A$ is the action name, $L$ is a literal (or ``head'') and $F$ is a, possibly empty, conjunction of literals.

\subsection{The AAA architecture}
\label{subsec:aaa_architecture}

The first logic-based agent architecture that we will discuss is the ``Autonomous Agent Architecture'' (AAA) architecture \cite{balduccini_aaa_2008}.
The AAA architecture is based on work in \cite{baral_reasoning_2000,balduccini_diagnostic_2003, balduccini_answer_2006,balduccini_learning_2007}.
As it is a logic-based approach, it makes certain assumptions about the environment in which an agent constructed in its architecture will be operating.
They must be met in order for the architecture to be applicable.
The assumptions are as follows \cite{balduccini_aaa_2008}:

\begin{itemize}
    \item The world (the agent along with its environment) can be modeled in a transition system.
    \item The agent is competent in interacting with its environment (e.g. the agent is capable of making correct observations, remembering them, and can perform actions).
    \item Normally, the agent can observe all relevant exogenous events (i.e. events that are not an effect of an action the agent performed).
\end{itemize}

Given that these assumptions are met, an agent in the AAA architecture performs the following loop once for every time step of the environment \cite{balduccini_aaa_2008}:

\begin{enumerate}
    \item Observe the world, explain observations, and update knowledge base.
    \item Select an appropriate goal G.
    \item Find a sequence of actions (called a \textit{plan}) that achieve G.
    \item Execute first action in plan and update knowledge base.
    \item Repeat (i.e. go to step 1).
\end{enumerate}

This loop is known as the \textit{Observe-Think-Act} loop \cite{balduccini_aaa_2008}.
The knowledge base referred to in this loop is encoded in the Answer Set Prolog (ASP) language, which allows for the representation of many different forms of knowledge \cite{balduccini_aaa_2008}.
ASP will be discussed later in section~\ref{sec:asp}.

There are two approaches to describing the effects of an agentâ€™s actions.
The first is to use action languages to formally denote the action effects, then to translate them into ASP for executability.
The second is to author them directly in ASP.
The authors of \cite{balduccini_aaa_2008} choose to do the latter.

It is worth noting that the agent makes no commitment to plans it found in previous loop iterations.
Each loop iteration searches for a plan irrespective to the plan that was partially executed in the previous iteration.
This makes the Observe-Think-Act loop somewhat greedy in that it chooses the best plan as can be foreseen at every time step.
\cite{blount_towards_2014} sees this as a limitation and builds the AIA architecture to address it.

\subsection{The AIA Architecture}
\label{subsec:aia_architecture}

As stated above, the ``Architecture for Intentional Agents'' (AIA) is an agent architecture that borrows significantly from the AAA architecture \cite{blount_towards_2014}.
It extends the AAA architecture by representing the possibility for action failure.
In contrast to the AAA architecture, the agent \textit{attempts} to perform an action during its control loop but may find that it is unable to do so.
In this case, the action is said to be not \textit{executable} \cite{blount_towards_2014}.

The AIA architecture shares a number of similar assumptions with the AAA architecture \cite{blount_towards_2014}:

\begin{itemize}
    \item The agent is capable of making correct observations.
    \item When the agent attempts to perform an action, the action occurs only when it is executable.
        Otherwise, nothing happens.
    \item The agent remembers its observations of the environment (as well as its prior attempts to take actions).
    \item Normally, the agent observes all relevant exogenous events.
\end{itemize}

However, a few assumptions differ from the AAA architecture:

\begin{itemize}
    \item The agent selects a single goal only when it does not have one and focuses on achieving it.
    \item The world must be modeled in an \textit{intentional system description} of AL.
\end{itemize}

\subsubsection{Intentional system description of AL}
\label{subsubsec:intentional_action_language_al}

An intentional system description of $ \mathcal{AL} $ is an extension of $ \mathcal{AL} $ proposed in \cite{blount_architecture_2013}.
It extends $ \mathcal{AL} $ by changing its signature from contains action to \textit{activities}.
Activities are the triple $<ActivityName,\ Plan,\ Goal>$, where $ActivityName$ is a name for the activity (typically a natural number), $Plan$ is a sequence of actions to achieve Goal.

\subsubsection{Theory of Intentions}
\label{subsubsec:theory_of_intentions}

\subsubsection{Examples}
\label{subsubsec:aia_examples}

Example 1

Example 2

Example 3

Example 4

Example 5

\subsection{Policies}
\label{subsec:policies}

\section{Answer Set Programming}
\label{sec:asp}
