\chapter{Background \& Related Work}

% This section is where you will discuss relevant background work, and related works for comparison.
% Ensure that you cite references appropriately, using this as an example~\cite{sample2019}
%
% \section{Background Topic 1}
%
% \section{Background Topic 2}
%
% \section{Related Work}

\section{Agents}

There has been prior research and discussion to determine what constitutes an ``agent''.
Though work in \cite{wooldridge_agent_1995} fails completely define the term,
the authors do propose a set of characteristics that agents must have.
They are summarized in \cite{dignum_intentional_1998} and are as follows.

First, the agent must have autonomy to complete its own actions without the intervention of a human.
This implies that it is making decisions on its own power.
Second, the choices of an agent must be best explained in terms of some form of \textit{intention}.
These intentions can be desires, goals, etc.
An implication of this requirement is that agents cannot be explained through low-level concepts.
Lastly, an agent explained with particular intentions should seem to actually progress the satisfaction of them.
For example, a system with the intention of achieving a goal,
but fails to do so when alternatives would have done so, cannot be called an ``agent''.

There are many different kinds of agents.
\cite{balke_how_2014} provides a survey of five different kinds of agents.

The first of which are production rule systems \cite{balke_how_2014}.
Production rule systems are symbolic in nature and
consist of three core components:

\begin{itemize}

    \item A set of \textit{rules} (also called \textit{productions}).
        Each rule has the form $ C_i \rightarrow A_i $,
        where $ C_i $ is what is called the \textit{sensory precondition}
        and $ A_i $ is the action to be performed if $ C_i $ is true.
        $ C_i $ is essentially an ``if'' condition and $ A_i $ is the conditionally executed body.

    \item A knowledge base that stores domain-relevant information about the agent's environment.
        This is called the \textit{working memory}.

    \item A \textit{rule interpreter} determines which rules apply for the current state of the working memory.
        In the case of rule conflicts, it decides which rule should be executed.

\end{itemize}

% TODO: Consider talking about architecture

Production rule systems are usually implemented in Prolog or LISP \cite{balke_how_2014}.

The next class of agents are those ascribing to the BDI architecture \cite{balke_how_2014}.
The belief-desire-intention (BDI) model combines a philosophical model of human practical reasoning
with several successful applications \cite{georgeff_belief-desire-intention_1999}.
BDI agents are built on the idea of mental states,
consisting of three items: beliefs, desires, and intentions \cite{balke_how_2014}.
Beliefs refer to an agent's representation of the world.
An agent believes something to be true if and only if the statement is found in its representation of the world.
It is worth noting that an agent's beliefs can be inconsistent with its environment.
In this case, the agent would be wrong.

\cite{rao_modeling_1991}

The desires of an agent are all possible courses of action that it might want to accomplish in order to reach a goal \cite{balke_how_2014}.
An agent is not necessarily committed to its desires.
They just play a part in its decision process.
Intentions, on the other hand, are commitments to particular courses of action to reach a goal.
Courses of action are also called plans.

In addition, BDI agents have a library of plans.
This library consists of pre-computed primitive logic rules to signify which actions contribute to accomplishing which goals.
At each reasoning step, BDI agents search through the plan library to see which plans have a post-condition that matches the currently selected intention
and then sorts them according to relevance.

The third class of agents are those that conform to normative models.
They differ from BDI agents in that they are externally motivated.
BDI agents are internally motivated since all major components (beliefs, goals, and intentions) are internal to the agent.
Normative agents are governed by norms, which are imposed by the agent's external environment.

The last two classes of agents are those with cognitive models \cite{balke_how_2014}.
They can be divided into those that have ``simple'' cognitive models and those that have ``cognitive architectures''.
The simple cognitive models share similarities with the agent classes presented above
whereas the cognitive architectures are heavily influenced by the structure of the human brain.

Given these five classes of agent architecture, one needs a way of objectively comparing them.
\cite{balke_how_2014} provides five different dimensions on which these architectures can be evaluated.

The first of which is the cognitive dimension.
This dimension considers what kind of reasoning is able to be performed by an agent.
\cite{balke_how_2014} provides two partitions for cognitive ability:
reactive or deliberative and composed of simple cognitive components or of psychologically/neurologically inspired ones.
Reactive agents are those that only respond to stimuli in their environment and typically follow rule-based patterns.
Deliberative agents, on the other hand, actively consider different courses of action and weigh the utility of each.
For example, production rule systems are an example of reactive agents whereas BDI agents can be deliberative \cite{balke_how_2014}.
Agents composed of psychologically/neurologically inspired cognitive components are those whose design strives to model the human brain,
whereas agents composed of simple cognitive components are those whose design consists of manmade abstractions.
Psychologically/neurologically agents correspond to the cognitive-model class mentioned above.

The second dimension for agent architecture comparisons is the affective dimension \cite{balke_how_2014}.
The affective dimension considers what degree of emotions agents in an architecture are capable of expressing and acting upon.
Research towards this dimension considers how emotions can be triggered, how they influence the decision process, and how changes in decisions affect the system at large.
Most models are incapable of acting upon emotions \cite{balke_how_2014} with the exception of a two extensions of the BDI model \cite{jiang_ebdi_2007, dignum_towards_2009} and a cognitive model \cite{urban_pecs_2000}.

A third dimension for comparing architectures is the social dimension.
This dimension considers the extent to which agent architectures provide agents the ability to communicate with other agents, to distinguish social relations (such as status), and to understand complex social concepts.
All the normative models discussed in \cite{balke_how_2014}, along with a few others among BDI extensions and cognitive models, have a notion of self, others, and groups.
BDI agents, in the original model, as well as production rule systems have no notion of social concepts or communication.

Fourth, agent architectures can be evaluated according to the extent of formal and social norms that agents can reason over \cite{balke_how_2014}.
Production rule systems

Lastly, architectures can be compared by considering the type of agent learning that the architecture enables for its agents \cite{balke_how_2014}.
This dimension considers whether agents can improve or expand their knowledge base on the basis of environmental observations.
If agents are capable of doing this, it also considers in what ways the knowledge base can be extended (e.g. adjusting the values for pre-existing decision functions or creating new decision rules from scratch) \cite{balke_how_2014}.
Production rule systems and BDI agents (including its variants) do not support any form of learning whereas normative and cognitive model support it to various extents.
